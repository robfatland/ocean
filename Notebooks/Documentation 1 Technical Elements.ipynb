{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f04978-c60b-42ca-8688-4fb4944bdfbc",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "This notebook is a technical review of the elements in this Jupyter notebok repository.\n",
    "\n",
    "\n",
    "* [Python Data Science Handbook by Jake VanderPlas]((https://jakevdp.github.io/PythonDataScienceHandbook/) (abbreviated herein PDSH).\n",
    "* [GitHub Markdown Cheat Sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)\n",
    "\n",
    "\n",
    "## Elements\n",
    "\n",
    "\n",
    "- Earlier organizational ideas\n",
    "    - Defers utility code to the supporting BioOpticsModule.py file\n",
    "    - Decomposition of pressure/time to infer **profile timing metadata**\n",
    "    - Daily profile chart illustrating nine profiles including noon/midnight variants\n",
    "    - **Depth** vs { **Temp, Dissolved Oxygen, Salinity, Chlor-A, FDOM (CDOM), backscatter** }\n",
    "    - (placeholder) Depth vs pCO2: midnight/noon descent\n",
    "    - **Depth vs pH**: midnight/noon descent\n",
    "    - **Depth vs { Spectral Irradiance }**: midnight/noon ascent \n",
    "    - **Depth vs PAR (continuous) and nitrate** (midnight/noon ascent)\n",
    "    - **Depth vs current** including turbulence\n",
    "    - Comparison: **Shallow profiler versus discrete CTD cast data from cruises**\n",
    "- Notebooks sub-folder\n",
    "    - Ocean 01 A: Parent of BioOptics: Slated to be subsumed / deleted\n",
    "    - Ocean 01 B/C/D/E/F: Likewise: Development code and docs, to be subsumed / deleted\n",
    "    - Ocean 01 G: Compares ascent data to descent data from continuous sensors\n",
    "    - Ocean 01 H: Compares Science Pod at rest on platform with platform sensors\n",
    "    - Ocean 01 K: Further background on bio-optical sensors (no spectrophotometer)\n",
    "    - Ocean 01 L: Initial attempts at differentiation with respect to depth\n",
    "    - Ocean 01 M: **Spectrophotometer**: Disabled pending source data refresh\n",
    "    - Ocean 02: Sea Surface = **MODIS Aqua surface chlorophyll**, broken pending refresh\n",
    "    - Ocean 03 **ARGO**: ARGO compared with RCA (nbk broken pending source data refresh) \n",
    "    - Ocean 04 Global Ocean: Depth-wise global explorer for GLODAP temp/salinity/DO (refresh needed)\n",
    "    - Ocean 05 A **Sea Floor Axial Inflation**: Tilt and pressure data at ASHES vent field\n",
    "    - Ocean 05 B Sea Floor Background: Backing resources / documentation for 05 A above\n",
    "    - Ocean 06 Science: Notes on global carbon cycle\n",
    "    - Ocean 07 Data Acquisition: Notes on getting datasets (refresh needed) \n",
    "    - Ocean 08 **Programming Resources**: Fairly careful notes on methods used in the above notebooks\n",
    "- Missing: OSU sea surface anomaly time series maps, animals\n",
    "\n",
    "- Import, play, modify sound files (broadband hydrophone)\n",
    "- PyGMT for mapping\n",
    "- Docker containers for processing data\n",
    "- Interactive data annotation using the cursor\n",
    "- Create / embed data animations and related video (including from external sources like YouTube)\n",
    "- Create / embed static images (particularly data charts)\n",
    "- Three-dimensional charting in Jupyter and non-Jupyter contexts\n",
    "- Shell integration built into IPython e.g. `Video(os.getcwd() + '/sub-path/data.mp4', embed=True)`\n",
    "- `numpy` / `pandas` / `xarray` / `matplotlib` / `holoview` / `ipywidgets` details and notes on effective use\n",
    "- Managing data; includes missing data and resampling\n",
    "- Obtaining datasets from OOI and other sources\n",
    "- Integrating remote sensing data\n",
    "- Time\n",
    "- Instruments (includes *sensors* and *streams* in OOI usage)\n",
    "- Shallow profiler \n",
    "    - ascent/descent/rest cycles\n",
    "    - low-rate instruments\n",
    "    - high-rate instruments\n",
    "- Ecosystem: bash, text editor, git, GitHub, Python, nbpuller, Binder\n",
    "    - Basic Python / IPython / notebook~module coding strategy\n",
    "    - Python `matplotlib`, `numpy`, `pandas`, and `xarray`\n",
    "    - interactive widgets, maps (`PyGMT`), animation, colormap use, ...\n",
    "    - Jupyter notebooks\n",
    "        - Tables, markdown, embedding, LaTeX, ...\n",
    "        - Server options\n",
    "- Ordering, retrieving and cleaning datasets from OOI\n",
    "    - Deconstructing datasets\n",
    "- Pulling other data (besides shallow profiler) from the OOI data system\n",
    "- Pulling datasets from other programs: ARGO, MODIS, GLODAP, ROMS, MSLA, etcetera\n",
    "- Working from larger extra-repo datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfb8780-009d-4116-aeb7-5bbf4da8d26c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## To Do\n",
    "\n",
    "\n",
    "- merge() ...?... \n",
    "    - Order: `.merge()` then `.resample()` with `mean()`; or vice versa? (existing code is vice-versa)\n",
    "    - This approach does resampling prior to merge but was taking way too long\n",
    "- resampling\n",
    "\n",
    "```\n",
    "ds = ds.reset_coords('seawater_pressure')        # converts the coordinate to a data variable\n",
    "ds_mean = ds.resample(time='1Min').mean()\n",
    "ds_std  = ds.resample(time='1Min').std()\n",
    "```\n",
    "\n",
    "- How to copy a dataset, how to move a coordinate to a data variable\n",
    "\n",
    "\n",
    "- Load R/M Dataset ctdpf + flort\n",
    "    * Some data are noisier towards the surface, some are ridiculously noisy. \n",
    "    * Idea: Filter on standard deviation, threshold:\n",
    "        * Filter leaves a signal of interest? \n",
    "    * Fluorometer is particularly troublesome. \n",
    "\n",
    "\n",
    "- Depth profiles reduce to simple metrics\n",
    "    * profile start, peak, end times\n",
    "    * platform residence: start and end times (from profile times)\n",
    "    * (smoothed) chlorophyll derivative, curvature, rate of curvature\n",
    "    * Similarly salinity seems to go through a consistent double-zero in rate of curvature\n",
    "    * intersection depth as used in TDR; for example for temperature or salinity\n",
    "        * extrapolate smoothed pressure by backing off the derivative change\n",
    "        * extrapolate platform, intersect\n",
    "    * time of day / sun angle\n",
    "    * local time\n",
    "    * rate of ascent verify; 3m / minute?\n",
    "    \n",
    "    \n",
    "    \n",
    "- platform coincident data agree with profiler\n",
    "- Deal with two older images:\n",
    "    - ./images/misc/optaa_spectra_0_10_20_JAN_2019.png\n",
    "    - ./images/misc/nitrate_2019_JAN_1_to_10.png\n",
    "- pH sensor fire once at the end of every profile; back in the platform***\n",
    "- Manufacturer etc: [here](https://interactiveoceans.washington.edu/instruments/).\n",
    "- ...but the DataArray can itself be invoked with `.attrs` to see additional attributes that are invisible\n",
    "\n",
    "```\n",
    "ds.density.attrs\n",
    "```\n",
    "\n",
    "- Optical absorption spectrophotometer\n",
    "    * Seabird Scientific (acquired WETLABS): 'AC-S' model (shallow profilers)\n",
    "        * 86 wavelengths per sample; in practice some nan values at both ends\n",
    "        * Interview Chris for more procedural / interpretive SME\n",
    "        * Operates only during shallow profiler ascents\n",
    "            * Only on the two \"nitrate\" ascents each day\n",
    "            * One sample per 0.27 seconds\n",
    "                * However it often does a \"skip\" with a sample interval about 0.5 seconds\n",
    "        * Spectral absorption: parameter `a`, values typically 20 - 45. \n",
    "        * Attenuation is `c` with values on 0 to 1.\n",
    "        * Coordinates we want are `time`, `int_ctd_pressure`, `wavelength`\n",
    "            * `time` and `wavelength` are also dimensions\n",
    "        * Data variables we want are `beam_attenuation` (this is `c`) and `optical_absorption` (`a`)\n",
    "        * Per year data is about 1.7 billion floating point numbers\n",
    "            * 86 wavelengths x 2 (c, a) x 2 (ascent / day) x 14,000 (sample / ascent) x 365\n",
    "\n",
    "        \n",
    "\n",
    "- Photosynthetically Active Radiation (PAR)\n",
    "    * Devices mounted on the shallow profiler and the SP platform\n",
    "    * Seabird Scientific (from acquisition of Satlantic): PAR model\n",
    "    * Some ambiguity in desired result: `par`, `par_measured` and `par_counts_output` are all present in the data file\n",
    "        * Since `qc` values are associated with it I will simply use `par_counts_output`\n",
    "        \n",
    "        \n",
    "- Fluorometer\n",
    "    * WETLABS (Seabird Scientific from acquisition) Triplet\n",
    "    * Chlorophyll emission is at 683 nm\n",
    "    * Measurement wavelengths in nm are 700.0 (scattering), 460.0 (cdom) and 695.0 (chlorophyll)\n",
    "    * Candidate Data variables\n",
    "        * Definites are `fluorometric_chlorophyll_a` and `fluorometric_cdom`\n",
    "        * Possibles are `total_volume_scattering_coefficient`, `seawater_scattering_coefficient`, `optical_backscatter`\n",
    "            * qc points to total volume scattering and optical backscatter but I'll keep all three\n",
    "  \n",
    "  \n",
    "- Nitrate nutnr_a_sample and nutnr_a_dark_sample\n",
    "    * The nitrate run ascent is ~62 minutes (ascent only); ~3 meters per minute\n",
    "        * Ascent is about 14,000 samples; so 220 samples per minute\n",
    "        * That is 70 samples per meter depth over 20 seconds\n",
    "        * Per the User's Manual post-processing gets rather involved\n",
    "\n",
    "\n",
    "- pCO2 water (two streams: pco2w_b_sami_data_record and pco2w_a_sami_data_record)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b81de1-0473-4b10-8ce1-8b67388bf480",
   "metadata": {},
   "source": [
    "## [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "* [Tables](#Tables)\n",
    "* [Data](#Data)\n",
    "* [Markdown](#Markdown)\n",
    "    * Includes [LaTeX math formulas](#LaTeX-math-formulas)\n",
    "* [Reducing Datasets](#Reducing-Datasets)\n",
    "    * [One minute resampling](#One-minute-resampling)\n",
    "    * [Selecting based on a range](#Selecting-based-on-a-range)\n",
    "* [Numpy ndarrays](#Numpy-ndarrays)\n",
    "* [Time](#Time)\n",
    "* [ipywidgets](#ipywidgets)\n",
    "* [HoloView](#Holoview)\n",
    "* [Instruments](#Instruments)\n",
    "* [Binder](#Binder)\n",
    "\n",
    "\n",
    "Misplaced:\n",
    "\n",
    "* [Plotting](#Plotting)\n",
    "    * [Making animations](#Making-animations)\n",
    "* [Multimedia](#Multimedia)\n",
    "    * [Images](#Images)\n",
    "    * [Animations](#Animations)\n",
    "    * [YouTube video playback](#YouTube-video-playback)\n",
    "    * [Sound clips](#Sound-clips)\n",
    "* [XArray Datasets and DataArrays](#XArray-Datasets-and-DataArrays)\n",
    "* [Pandas Series and DataFrames](#Pandas-Series-and-DataFrames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45608b7c-07fa-46f1-bad4-ea1c2baa68bf",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "## Tables\n",
    "\n",
    "\n",
    "Possible finesse: Left-justify tables: `<style>table {float:left}</style>`\n",
    "\n",
    "\n",
    "Can be done in Markdown using pipes; or using HTML. Pipe tables with many columns can get buggy.\n",
    "In Python cells: HTML via `%%html` cell magic.\n",
    "\n",
    "| Tables        | Are           | Cool  |\n",
    "| ------------- |:-------------:| -----:|\n",
    "| col 3 is      | right-aligned | \\$1600 |\n",
    "| col 2 is      | centered      |   \\$12 |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef03dbf0-48bb-4df9-9f42-6b307936dc1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<br>\n",
       "Here is a table and additional HTML content... from within a Python cell using %%html cell magic.\n",
       "<br>\n",
       "\n",
       "<!DOCTYPE html>\n",
       "<html>\n",
       "\n",
       "<body>\n",
       "    <table>\n",
       "        <tr>\n",
       "            <th>Book</th>\n",
       "            <th>Author</th>\n",
       "            <th>Genre</th>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Thief</td>\n",
       "            <td>Zusak</td>\n",
       "            <td>Made It Up</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>$x = \\pi$</td>\n",
       "            <td>Holly Berry</td>\n",
       "            <td>Mathematics</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Silence</td>\n",
       "            <td>Burden of Dreams</td>\n",
       "            <td>Climbing</td>\n",
       "        </tr>\n",
       "    </table>\n",
       "</body>\n",
       "\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<br>\n",
    "Here is a table and additional HTML content... from within a Python cell using %%html cell magic.\n",
    "<br>\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "\n",
    "<body>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Book</th>\n",
    "            <th>Author</th>\n",
    "            <th>Genre</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Thief</td>\n",
    "            <td>Zusak</td>\n",
    "            <td>Made It Up</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>$x = \\pi$</td>\n",
    "            <td>Holly Berry</td>\n",
    "            <td>Mathematics</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Silence</td>\n",
    "            <td>Burden of Dreams</td>\n",
    "            <td>Climbing</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</body>\n",
    "\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd59350-698b-4cc9-af6d-a2905026042b",
   "metadata": {},
   "source": [
    "<br>\n",
    "Here is the same table and additional HTML content... from within a markdown cell\n",
    "<br>\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "\n",
    "<body>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Book</th>\n",
    "            <th>Author</th>\n",
    "            <th>Genre</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Thief</td>\n",
    "            <td>Zusak</td>\n",
    "            <td>Made It Up</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>$x = \\pi$</td>\n",
    "            <td>Holly Berry</td>\n",
    "            <td>Mathematics</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Silence</td>\n",
    "            <td>Burden of Dreams</td>\n",
    "            <td>Climbing</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</body>\n",
    "\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746498ae-a9fe-4863-8627-cf6a4d033421",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "\n",
    "The primary data resource in this repository is the NSF-sponsored Ocean Observations Initiative (OOI) program.\n",
    "This consists of a set of about eight *arrays*, which can be thought of as independent observatories. \n",
    "\n",
    "\n",
    "### Data download\n",
    "\n",
    "\n",
    "[OOI](https://oceanobservatories.org) provides the [Data Explorer](https://dataexplorer.oceanobservatories.org/).\n",
    "An alternative resource that is particular to the *OOI cabled array* is the\n",
    "[Cabled Array data server](https://interactiveoceans.washington.edu).\n",
    "This site is oriented towards built-in data exploration and it is very educator-friendly. \n",
    "\n",
    "\n",
    "#### Example download\n",
    "\n",
    "\n",
    "- Source: OOI\n",
    "    - Qualifiers: Fluorometer (Chlorophyll), March 2021 example, Oregons Slope Base site, shallow profiler\n",
    "- > [Data access page in the Data Explorer](https://dataexplorer.oceanobservatories.org/#go-to-data-access)\n",
    "    - > Oregon Margin, Profiling Assets, Fluorometer, All Parameters (+ Go)\n",
    "- Five choicie tabs: **Data**, **Inventory**, **Downloads**, **Annotations**, **Deployments**\n",
    "    - > **Downloads**\n",
    "    - > green **Downloads** button for { Oregon Slope Base, Shallow Profiler, Chlorophyll-A}\n",
    "    - Choose link '*full resolution downloads*'\n",
    "        - This leads to a page of datasets for the fluorometer in sequence of deployment, giving some time-range choice\n",
    "        - Select Downloads > THREDDS Catalog > Dataset to go to a page listing files\n",
    "        - Deployment 8, FLORT for the triplet; check time range in filename for best match to March 2021\n",
    "        - Click to go to yet another page, this one for \"ACCESS\" and multiple choices\n",
    "            - Select HTTPServer to download the NetCDF data file: 600+ MB\n",
    "    - Raw data alternative to **full resoliution**\n",
    "        - Green **Download** button for various streams / sensors \n",
    "        - The Download button presents three format options: ERDDAP (with 3 sub-options), CSV and NetCDF\n",
    "            - The NetCDF URL copy gives \n",
    "[this link](https://erddap.dataexplorer.oceanobservatories.org/erddap/tabledap/ooi-rs01sbps-sf01a-3a-flortd101.nc?time%2Cmass_concentration_of_chlorophyll_a_in_sea_water_profiler_depth_enabled%2Cz)\n",
    "which in turn initiates a download, making the download link shareable\n",
    "            - The NetCDF **Download** button downloads a downsampled data file to the local machine\n",
    "                - In this case the file size is 184MB; and these data are ***one sample per minute***\n",
    "                - For completeness we might also download the FDOM (\"CDOM\") and Optical Backscatter files (same size)\n",
    "            - At no point did we constrain the data to a particular time range: The data run 2014 to 2022\n",
    "            - The data description text with the small chart symbol at left is also a link: This attempts to create a curtain plot in the browser\n",
    "            \n",
    "            \n",
    "### Data manipulation\n",
    "\n",
    "#### XArray Dataset operations\n",
    "\n",
    "\n",
    "- Copy: `dsc = ds.copy()`\n",
    "- Coordinate to data variable: `ds = ds.reset_coords('seawater_pressure')`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35751943-2d7d-4b1d-8064-60a7ed356b05",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "## Markdown\n",
    "\n",
    "See above for tables. This continues with some other examples.\n",
    "\n",
    "\n",
    "<BR>\n",
    "<img src=\"../Images/people/dubious.png\" style=\"float: left;\" alt=\"picture: dubious child\" width=\"120\"/>\n",
    "<div style=\"clear: left\">\n",
    "<BR><BR>\n",
    "    \n",
    "\n",
    "\n",
    "- bullet\n",
    "- lists\n",
    "\n",
    "\n",
    "```\n",
    "'blocks of code...'\n",
    "# with fixed font width\n",
    "```\n",
    "\n",
    "\n",
    "> \"quotation-style\" text\n",
    "\n",
    "\n",
    "\n",
    "#### Needed: Embed YouTube videos\n",
    "\n",
    "    \n",
    "### LaTeX math formulas \n",
    "\n",
    "$LaTeX^{\\pi}$\n",
    "    \n",
    "Single dollar-sign delimiters put content inline: `$e^x = \\sum_{i=0}^{\\infty}{\\frac{x^i}{i!}}$` looks like this: $e^x = \\sum_{i=0}^{\\infty}{\\frac{x^i}{i!}}$. \n",
    "\n",
    "\n",
    "Double dollar-sign delimiters create a centered equation:\n",
    "\n",
    "\n",
    "$$e^x = \\sum_{i=0}^{\\infty}{\\frac{x^i}{i!}}$$\n",
    "\n",
    "\n",
    "Change size for example with a `\\Large` qualifier:\n",
    "\n",
    "\n",
    "$\\Large{e^x = \\sum_{i=0}^{\\infty}{\\frac{x^i}{i!}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae0fd4-1bb0-4386-a831-0070aac487b6",
   "metadata": {},
   "source": [
    "\n",
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "\n",
    "## Reducing Datasets\n",
    "\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "\n",
    "As a good reference for manipulating data see Jake VanDerplas' book\n",
    "Python Data Science Handbook. \n",
    "\n",
    "\n",
    "We have here multi-dimensional oceanography datasets bundled as\n",
    "NetCDF or CSV files. Corresponding Python modules are `XArray` and `pandas`.\n",
    "On import these are abbreviated `xr` and `pd` respectively.\n",
    "\n",
    "\n",
    "XArray has a method `.open_dataset('file.nc')` returning an XArray Dataset. \n",
    "A Dataset is a collection of XArray DataArray structures. The important structural\n",
    "feature of a Dataset is that it includes four sections: `Dimensions`, \n",
    "`Coordinates`, `Data Variables`, and `Attributes`. To examine a Dataset\n",
    "called `A`: Create an empty cell, type `A`, and run it. This will give a \n",
    "breakdown of `A` in terms of these four constituent sections. \n",
    "\n",
    "\n",
    "In pandas the data structure of interest is a DataFrame. DataFrames are used \n",
    "extensively in these notebooks to manage profile metadata for shallowo profilers.\n",
    "For more on this see the accompanying Technical Guide. \n",
    "\n",
    "\n",
    "Common reductive steps once data are read include removing extraneous components from\n",
    "a dataset, downsampling, removing NaN values, changing the primary `dimension`\n",
    "from `obs` (for 'observation') to `time`, combining multiple data files into \n",
    "a single dataset, saving modified datasets to new files, and creating data charts. \n",
    "\n",
    "\n",
    "One reason datasets are reduced in size is to enable them to reside within this\n",
    "[GitHub repository](https://github.com/robfatland/ocean)\n",
    "without exceeding memory size limits. If large datasets are still\n",
    "needed, they can be downloaded by a notebook. This is a further means of reducing\n",
    "repo size. An example of this download-as-needed using `wget`\n",
    "is in the **`Global Ocean`** notebook.\n",
    "The five data files downloaded used in that notebook are each 17MB, reduced \n",
    "from 100MB by the following Python code which retains only data of interest.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# Reduce volume of an XArray Dataset with extraneous Data Variables:\n",
    "T=xr.open_dataset('glodap_oxygen.nc')\n",
    "T.nbytes\n",
    "T=T[['temperature', 'Depth']]\n",
    "T.nbytes\n",
    "T.to_netcdf('temperature.nc')      \n",
    "```\n",
    "\n",
    "\n",
    "Specific to the RCA shallow profilers: Data collected during ascent spans 200 meters\n",
    "of water column depth in one hour, about 6 cm / sec. To perceive\n",
    "a 'thin layer' signal suggests starting from high rate data (example: one sample\n",
    "/ sec) and averaging -- if at all -- to a guess at minimum layer thickness.\n",
    "More in the spirit of XArray would be to resample based on depth bins.\n",
    "\n",
    "\n",
    "As an aside, one of the challenges of working with complex data formats such as\n",
    "XArray Datasets is learning how to access data subsets. While this repository \n",
    "is not a comprehensive guide it does include a number of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a5517b-4edf-453f-bf3e-94005d3fb05f",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "### Oregon Slope Base shallow profiler sensor abbreviation table \n",
    "\n",
    "\n",
    "| abbrev | sensors |remarks|\n",
    "|--|--|--|\n",
    "|ctdpf|3|CTD: includes salinity, temperature, dissolved oxygen\n",
    "|pco2|1|carbonate chemistry, midnight and noon *descent* only\n",
    "|phsen|1|pH, midnight and noon *descent* only\n",
    "|nutnr|2|nitrate, dark samples; midnight and noon *ascent* only\n",
    "|flort|3|Fluorometer triplet: chlorophyll-A, FDOM, backscatter ('bb700')\n",
    "|spkir|7|downwelling spectral irradiance, 7 frequency bands\n",
    "|parad|1|photosynthetically available radiation 'PAR'\n",
    "|optaa|86|spectrophotometer: 86 frequency bands, ascent local noon and midnight only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-rabbit",
   "metadata": {},
   "source": [
    "### Slow resampling problem\n",
    "\n",
    "\n",
    "The shallow profiler spectrophotometer has 86 channels. Each observation has \n",
    "a corresponding depth and time, typically several thousand per profile.\n",
    "The XArray Dataset has `time` swapped in for `obs` dimension but we are \n",
    "interested in resampling into depth bins. This took hours; which was \n",
    "puzzling. However page 137 of PDSH, on **Rearranging Multi-Indices**\n",
    "and **Sorted and unsorted indices** provides this resolution: \n",
    "\n",
    "> ***Rearranging Multi-indices***<BR>\n",
    "One of the keys to working with multiply indexed data is knowing how to effectively \n",
    "transform the data. There are a number of operations that will preserve all the \n",
    "information in the dataset, but rearrange it for the purposes of various computations. \n",
    "[...] There are many [ways] to finely control the rearrangement\n",
    "of data between heirarchical indices and columns.\n",
    "    \n",
    "> ***Sorted and unsorted indices***<BR>\n",
    "Earlier, we briefly mentioned a caveat, but we should emphasize it more here. \n",
    "*Many of the `MultiIndex`slicing operations will fail if the index is not sorted.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-retro",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "### One minute resampling\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "`XArray Datasets` feature selection by time range: `ds.sel(time=slice(timeA, timeB))`\n",
    "and resampling by time interval: `ds.resample(time='1Min').mean()`. \n",
    "(Substitute `.std()` to expand into standard deviation signals.)\n",
    "\n",
    "\n",
    "```\n",
    "ds = xr.open_dataset(ctd_data_filename)\n",
    "tJan1 = dt64('2019-01-01')\n",
    "tFeb1 = dt64('2019-02-01')\n",
    "ds = ds.sel(time=slice(tJan1, tFeb1))\n",
    "ds1Min = ds.resample(time='1Min').mean()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "The problem however is that the resample() execution in the code above\n",
    "can hang. The select operation `.sel()` is not understood by XArray as a monotonic\n",
    "time dimension monotonic. It may be treated as a jumble even if it is not! \n",
    "This can become even more catastrophic when other dimensions are present. \n",
    "The following work-around uses `pandas Dataframes`. \n",
    "\n",
    "\n",
    "\n",
    "This code moves the \n",
    "XArray Dataset contents into a pandas DataFrame.\n",
    "Here they are resampled properly; and the resulting\n",
    "columns are converted into a list of XArray DataArrays.\n",
    "These are then combined to form a new Dataset with \n",
    "the desired resampling completed quickly. \n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "df   = ds.to_dataframe().resample(\"1Min\").mean()\n",
    "vals = [xr.DataArray(data=df[c],                  \\\n",
    "                     dims=['time'],               \\\n",
    "                     coords={'time':df.index},    \\\n",
    "                     attrs=ds[c].attrs)           \\\n",
    "           for c in df.columns]\n",
    "ds = xr.Dataset(dict(zip(df.columns, vals)), attrs=ds.attrs)\n",
    "ds.to_netcdf('new_data_file.nc')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-boating",
   "metadata": {},
   "source": [
    "Flourometry code redux: For OSB shallow profiler triplet, to 1Min samples, JAN 2019\n",
    "\n",
    "\n",
    "```\n",
    "ds_Fluorometer = xr.open_dataset('/data/rca/fluorescence/osb_sp_flort_2019.nc')\n",
    "time_jan1, time_feb1 = dt64('2019-01-01'), dt64('2019-02-01')\n",
    "ds_Fluor_jan2019 = ds_Fluorometer.sel(time=slice(time_jan1, time_feb1))\n",
    "df               = ds_Fluor_jan2019.to_dataframe().resample(\"1Min\").mean()\n",
    "vals             = [xr.DataArray(data=df[c], dims=['time'], coords={'time':df.index}, \\\n",
    "                    attrs=ds_Fluor_jan2019[c].attrs) for c in df.columns]\n",
    "xr.Dataset(dict(zip(df.columns, vals)), \\\n",
    "           attrs=ds_Fluor_jan2019.attrs).to_netcdf('./data/rca/fluorescence/osb_sp_fluor_jan2019.nc')\n",
    "```\n",
    "\n",
    "Spectral irradiance stopgap version: Break out by spectrum (should be dimension of just one file).\n",
    "\n",
    "```\n",
    "spectral_irradiance_source = '/data/rca/irradiance/'\n",
    "spectral_irradiance_data = 'osb_sp_spkir_2019.nc'\n",
    "ds_spectral_irradiance = xr.open_dataset(spectral_irradiance_source + spectral_irradiance_data)\n",
    "ds_spectral_irradiance\n",
    "time_jan1, time_feb1 = dt64('2019-01-01'), dt64('2019-02-01')\n",
    "ds_Irr_jan2019 = ds_spectral_irradiance.sel(time=slice(time_jan1, time_feb1))\n",
    "df = [ds_Irr_jan2019.sel(spectra=s).to_dataframe().resample(\"1Min\").mean() for s in ds_Irr_jan2019.spectra]\n",
    "r = [xr.Dataset(dict(zip(q.columns, \n",
    "                         [xr.DataArray(data=q[c], dims=['time'], coords={'time':q.index}, \\\n",
    "                                       attrs=ds_Irr_jan2019[c].attrs) for c in q.columns] \\\n",
    "                    )   ), \n",
    "                attrs=ds_Irr_jan2019.attrs)\n",
    "    for q in df]\n",
    "for i in range(7): r[i].to_netcdf('./data/rca/irradiance/osb_sp_irr_spec' + str(i) + '.nc')\n",
    "```\n",
    "\n",
    "\n",
    "Spectral irradiance related skeleton code showing use of `.isel(spectra=3)`: \n",
    "\n",
    "\n",
    "```\n",
    "ds = ds_spkir.sel(time=slice(time0, time1))\n",
    "da_depth = ds.int_ctd_pressure.resample(time='1Min').mean()\n",
    "dsbar = ds.resample(time='1Min').mean()\n",
    "dsstd = ds.resample(time='1Min').std()\n",
    "dsbar.spkir_downwelling_vector.isel(spectra=3).plot()\n",
    "\n",
    "\n",
    "plot_base_dimension = 4\n",
    "indices = [0, 1, 2, 3, 4, 5, 6]\n",
    "n_indices = len(indices)\n",
    "da_si, da_st = [], []\n",
    "\n",
    "\n",
    "for idx in indices: \n",
    "    da_si.append(dsbar.spkir_downwelling_vector.isel(spectra=idx))\n",
    "    da_st.append(dsstd.spkir_downwelling_vector.isel(spectra=idx))\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(n_indices, 2, figsize=(4*plot_base_dimension, plot_base_dimension*n_indices), /\n",
    "           sharey=True, tight_layout=True)\n",
    "\n",
    "\n",
    "axs[0][0].scatter(da_si[0], da_depth, marker=',', s=1., color='k') \n",
    "axs[0][0].set(ylim = (200., 0.), xlim = (-.03, .03), title='spectral irradiance averaged')\n",
    "axs[0][1].scatter(da_st[0], da_depth, marker=',', s=1., color='r')\n",
    "axs[0][1].set(ylim = (200., 0.), xlim = (0., .002), title='standard deviation')\n",
    "\n",
    "\n",
    "for i in range(1, n_indices):\n",
    "    axs[i][0].scatter(da_si[i], da_depth, marker=',', s=1., color='k')\n",
    "    axs[i][0].set(ylim = (200., 0.), xlim = (-.03, .03))\n",
    "    axs[i][1].scatter(da_st[i], da_depth, marker=',', s=1., color='r')\n",
    "    axs[i][1].set(ylim = (200., 0.), xlim = (0., .002))\n",
    "```\n",
    "\n",
    "Code for PAR\n",
    "\n",
    "```\n",
    "par_source = '/data/rca/par/'\n",
    "par_data = 'osb_sp_parad_2019.nc'\n",
    "ds_par = xr.open_dataset(par_source + par_data)\n",
    "time_jan1 = dt64('2019-01-01')\n",
    "time_feb1 = dt64('2019-02-01')\n",
    "ds_par_jan2019 = ds_par.sel(time=slice(time_jan1, time_feb1))\n",
    "df   = ds_par_jan2019.to_dataframe().resample(\"1Min\").mean()\n",
    "vals = [xr.DataArray(data=df[c], dims=['time'], coords={'time':df.index}, attrs=ds_par_jan2019[c].attrs) for c in df.columns]\n",
    "ds_par_jan2019_1Min = xr.Dataset(dict(zip(df.columns, vals)), attrs=ds_par_jan2019.attrs)\n",
    "osb_par_nc_file = \"./data/rca/par/osb_sp_par_jan2019.nc\"\n",
    "ds_par_jan2019_1Min.to_netcdf(osb_par_nc_file)\n",
    "```\n",
    "\n",
    "PAR view: during shallow profiler rise/fall sequences\n",
    "\n",
    "```\n",
    "t0, t1 = '2019-07-17T13', '2019-07-18T05'\n",
    "t0, t1 = '2019-07-17T18:40', '2019-07-17T19:40'\n",
    "t0, t1 = '2019-07-17T21', '2019-07-17T23:00'        # These are the nitrate profiles\n",
    "t0, t1 = '2019-07-18T21', '2019-07-18T23:00'\n",
    "t0, t1 = '2019-07-19T21', '2019-07-19T23:00'\n",
    "t0, t1 = '2019-07-17T18:40', '2019-07-17T19:40'     # These are the profiles prior to nitrate\n",
    "t0, t1 = '2019-07-18T18:40', '2019-07-18T19:40'\n",
    "t0, t1 = '2019-07-19T18:40', '2019-07-19T19:40'\n",
    "da = ds_parad.sel(time=slice(t0, t1)).par_counts_output\n",
    "p=da.plot.line(marker='o', figsize = (14,8), markersize=1, yincrease = True)\n",
    "```\n",
    "\n",
    "Staged 'nitrate' profile compared with 'normal' profile\n",
    "\n",
    "```\n",
    "t0, t1 = '2019-07-19T20:30', '2019-07-19T23:50'               # USE THIS!! This is a good nitrate profile time bracket\n",
    "t0, t1 = '2019-07-19T18:40', '2019-07-19T19:40'\n",
    "da = ds_parad.sel(time=slice(t0, t1)).int_ctd_pressure\n",
    "p=da.plot.line(marker='o', figsize = (14,8), markersize=1, yincrease = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-moses",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "\n",
    "## Plotting \n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "### Summary\n",
    "\n",
    "Here I use [**`pyplot`**](https://plotly.com/python/), the Python graphing library usually imported as `plt`. \n",
    "Plotly is part of the matplotlib package. It is super confusing unless one has\n",
    "an extensive block of time to study it with care. In particular\n",
    "we need to be experts at \"drawing charts\" both with `.scatter` and `.plot`. Furthermore\n",
    "there is a quick-and-dirty `.plot` within XArray Datasets that can save time during\n",
    "development by giving a quick sanity check. In this section I will cover the basics \n",
    "of charting data in this context.\n",
    "\n",
    "### To investigate\n",
    "\n",
    "* Jake talks about Seaborn in PDSH; worth a look\n",
    "\n",
    "\n",
    "### Event handling and debugging\n",
    "\n",
    "\n",
    "See the Annotate (as of late 2022) notebook on creating an event handler for interactivity. \n",
    "Key idea: Declare a variable in the event handler a `global` so that after it has run at \n",
    "least once: That variable can be examined in a separate cell. Note: The variable need not \n",
    "be declared or used *outside* the event handler for this to work. \n",
    "\n",
    "\n",
    "### Sharing an axis\n",
    "\n",
    "\n",
    "Suppose a figure has an axis or a list of axes. These have a method `.twiny()` which creates\n",
    "a copy that can have its own x-axis stipulated. Same thing for `.twinx()`. This is demonstrated\n",
    "in the **Ocean 01 D Photic Zone Reduction** notebook. It risks cluttering the chart with the idea\n",
    "of condensing information.\n",
    "\n",
    "### Grid of charts\n",
    "\n",
    "This is example code for time-series data. It sets up a 3 x 3 grid of charts. These are matched to a 2-D set of\n",
    "axes (the 'a' variable) with both the scatter() and plot() constructs.\n",
    "\n",
    "```\n",
    "rn = range(9); rsi = range(7)\n",
    "\n",
    "p,a=plt.subplots(3, 3, figsize=(14,14))    # first 3 is vertical count, second 3 is horizontal count\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "a[0,0].plot(ctdF.time, ctdF.depth, color='r');                                  a[0,0].set(ylim=(200.,0.), title='Depth')\n",
    "a[0,1].plot(ctdF.time, ctdF.salinity, color='k');                               a[0,1].set(title='Salinity')\n",
    "a[0,2].plot(ctdF.time, ctdF.temperature, color='b');                            a[0,2].set(title='Temperature')\n",
    "a[1,0].plot(ctdF.time, ctdF.dissolved_oxygen, color='b');                       a[1,0].set(title='Dissolved Oxygen')\n",
    "a[1,1].scatter(phF.time.values, phF.ph_seawater.values, color='r');             a[1,1].set(title='pH')\n",
    "a[1,2].scatter(nitrateF.time.values, nitrateF.scn.values, color='k');           a[1,2].set(title='Nitrate')\n",
    "a[2,0].plot(parF.time, parF.par_counts_output, color='k');                      a[2,0].set(title='Photosynthetic Light')\n",
    "a[2,1].plot(fluorF.time, fluorF.fluorometric_chlorophyll_a, color='b');         a[2,1].set(title='Chlorophyll')\n",
    "a[2,2].plot(siF.time, siF.si0, color='r');                                      a[2,2].set(title='Spectral Irradiance')\n",
    "\n",
    "a[2,0].text(dt64('2017-08-21T07:30'), 155., 'local midnight', rotation=90, fontsize=15, color='blue', fontweight='bold')\n",
    "a[2,2].text(dt64('2017-08-21T07:30'), 4.25, 'local midnight', rotation=90, fontsize=15, color='blue', fontweight='bold')\n",
    "\n",
    "tFmt   = mdates.DateFormatter(\"%H\")                 # an extended format for strftime() is \"%d/%m/%y %H:%M\"\n",
    "t0, t1 = ctdF.time[0].values, ctdF.time[-1].values  # establish same time range for each chart\n",
    "tticks = [dt64('2017-08-21T06:00'), dt64('2017-08-21T12:00'), dt64('2017-08-21T18:00')]\n",
    "\n",
    "for i in rn: j, k = i//3, i%3; a[j, k].set(xlim=(t0, t1),xticks=tticks); a[j, k].xaxis.set_major_formatter(tFmt)\n",
    "print('')\n",
    "```\n",
    "\n",
    "\n",
    "Please note that Software Carpentry (Python) uses a post-facto approach to axes. \n",
    "In what follows there is implicit use of numpy 'collapse data along a particular\n",
    "dimension' using the `axis` keyword. So this is non-trivial code; but main point \n",
    "it shows adding axes to the figure.\n",
    "\n",
    "```\n",
    "fig = plt.figure(figsize=(10,3))\n",
    "\n",
    "axes1 = fig.add_subplot(1,3,1)\n",
    "axes2 = fig.add_subplot(1,3,2)\n",
    "axes3 = fig.add_subplot(1,3,3)\n",
    "\n",
    "avg_data = numpy.mean(data, axis=0)\n",
    "min_data = numpy.min(data, axis=0)\n",
    "max_data = numpy.max(data, axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-contributor",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "### Making animations\n",
    "\n",
    "[Top](#Introduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca32a7",
   "metadata": {},
   "source": [
    "This section was lifted from the BioOptics.ipynb notebook and simplified. It illustrates **overloading** a chart to \n",
    "show multiple sensor profiles evolving over time (frames). It also illustrates using markers along a line plot to\n",
    "emphasize observation spacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "qualified-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This code creates the animation; requires some time so it is commented out for now.\n",
    "# anim = animation.FuncAnimation(fig, AnimateChart, init_func=AnimateInit, \\\n",
    "#                                frames=nframes, interval=250, blit=True, repeat=False)\n",
    "#\n",
    "# Use 'HTML(anim.to_html5_video())'' for direct playback\n",
    "# anim.save(this_dir + '/Images/animations/multisensor_animation.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c230db14",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "\n",
    "## Binder-friendly playback\n",
    "\n",
    "\n",
    "The cell above creates an animation file that is stored within this repository. \n",
    "The cell below plays it back (for example in **binder**) to show multiple profile animations.\n",
    "Nitrate is intermittent, appearing as a sky-blue line in 2 of every 9\n",
    "frames. The remaining sensors are present in each frame.\n",
    "\n",
    "\n",
    "There animation begins March 1 2021 and proceeds at a rate of nine frames (profiles) per day.\n",
    "Change playback speed using the video settings control at lower right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "raised-romantic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video controls  >\n",
       " <source src=\"data:None;base64,/home/kilroy/ocean/Notebooks../Images/animations/multisensor_animation.mp4\" type=\"None\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binder-friendly playback\n",
    "from IPython.display import HTML, Video\n",
    "import os\n",
    "Video(os.getcwd() + '../Images/animations/multisensor_animation.mp4', embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-parker",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "## Multimedia\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "\n",
    "<BR>\n",
    "<img src=\".././Images/fauna/dubious.png\" style=\"float: left;\" alt=\"dubious person trying to eat kelp\" width=\"100\"/>\n",
    "<div style=\"clear: left\">\n",
    "<BR>\n",
    "\n",
    "This png file of a child dubiously eating kelp illustrates transparent pixels. \n",
    "There are lots of free/native apps to do this. \n",
    "    \n",
    "    \n",
    "### Summary\n",
    "\n",
    "We want to include (in order of importance) images, animations of data and sound clips. The above image,\n",
    "incidentally, includes transparent pixels: Done on a PC using Paint 3D, where the process is a bit convoluted. \n",
    "\n",
    "### Images\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "The cleanest presentation for static images in my experience so far is to use HTML in a \n",
    "markdown cell. It looks as follows where `<BR>` is a line break giving some spacing. Note\n",
    "the relative path.\n",
    "\n",
    "```\n",
    "<BR>\n",
    "<img src=\"./../Images/vessels/revelle.jpg\" style=\"float: left;\" alt=\"ship and iceberg photo\" width=\"900\"/>\n",
    "<div style=\"clear: left\">\n",
    "<BR>\n",
    "```\n",
    "\n",
    "It is possible to include images in Python cells using PIL but it was more of a chore.\n",
    "\n",
    "\n",
    "### Animations\n",
    "    \n",
    "    \n",
    "[Top](#Introduction)\n",
    "\n",
    "    \n",
    "Once an mp4 file is written to the file system playback is simple: Import `Video` from `IPython.display`\n",
    "and play the file back using `Video` setting the `embed` flag True.\n",
    "\n",
    "\n",
    "```\n",
    "from IPython.display import Video\n",
    "Video(\"./<some_animation>.mp4\", embed=True)\n",
    "```\n",
    "\n",
    "Alternative to `, embed=True`: Turn on the inline back-end using `%matplotlib inline` line magic\n",
    "\n",
    "    \n",
    "#### YouTube video playback\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "    \n",
    "```\n",
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('sjfsUzECqK0')\n",
    "```\n",
    "    \n",
    "### Sound clips\n",
    "\n",
    "    \n",
    "[Top](#Introduction)\n",
    "\n",
    "```\n",
    "from IPython.display import Audio\n",
    "Audio(\"<audiofile>.mp3\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-spectrum",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "## XArray Datasets and DataArrays\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "There are a million little details about working with XArray Datasets, DataArrays, numpy arrays, pandas DataFrames,\n",
    "pandas arrays... let's begin! The main idea is that a **DataArray** is an object containing, in the spirit of \n",
    "the game, one sort of data; and a **Dataset** is a collection of associated **DataArray**s. \n",
    "\n",
    "\n",
    "### XArray ***Dataset*** basics\n",
    "\n",
    "**Datasets** abbreviated `ds` have components { dimensions, coordinates, data variables, \n",
    "attributes }.\n",
    "\n",
    "\n",
    "A **DataArray** relates to a **`name`**; needs elaboration. \n",
    "\n",
    "\n",
    "```\n",
    "ds.variables\n",
    "\n",
    "ds.data_vars                                  # 'dict-like object'\n",
    "\n",
    "for dv in ds.data_vars: print(dv)\n",
    "    \n",
    "choice = 2\n",
    "this_data_var = list(ds.data_vars)[choice]\n",
    "print(this_data_var)\n",
    "\n",
    "ds.coords\n",
    "ds.dims\n",
    "ds.attrs\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### Load via `open_mfdataset()` with dimension swap from `obs` to `time`\n",
    "\n",
    "\n",
    "A single NetCDF (`.nc`) file can be opened as an XArray Dataset using `xr.open_dataset(fnm)`. \n",
    "Multiple files can be opened as a single XArray Dataset via `xr.open_mfdataset(fnm*.nc)`. \n",
    "`mf` stands for `multi-file`. Note \n",
    "the wildcard `fnm*` is supported. \n",
    "\n",
    "```\n",
    "def my_preprocessor(fds): return fds.swap_dims({'obs':'time'})\n",
    "\n",
    "ds = xr.open_mfdataset('files*.nc',                                \\\n",
    "                       preprocess = my_preprocessor,             \\\n",
    "                       concat_dim='time', combine='by_coords')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-compound",
   "metadata": {},
   "source": [
    "#### Obstacle: Getting information out of a Dataset\n",
    "\n",
    "There is a sort of comprehension / approach that I have found hard to internalize.\n",
    "With numpy ndarrays, XArray Datasets, etcetera there is this \"how do I get at it?\"\n",
    "problem. As this documentation evolves I will try and articulate the most helpful\n",
    "mindset. The starting point is that Datasets are built as collections of DataArrays; \n",
    "and these have an indexing protocol the merges with a method protocol (`sel`, `merge`\n",
    "and so on) where the end-result code that does what I want is inevitably very \n",
    "elegant. So it is a process of learning that elegant sub-language...\n",
    "\n",
    "\n",
    "#### Recover a time value as `datetime64` from a Dataset by index\n",
    "\n",
    "If `time` is a `dimension` it can be referenced via `ds.time[i]`. However\n",
    "this will be a 1-Dimensional, 1-element DataArray. Adding `.data`\n",
    "and casting the resulting ndarray (with one element) as a `dt64` works.\n",
    "\n",
    "```dt64(ds.time[i].data)```\n",
    "\n",
    "\n",
    "#### Example: XArray transformation flow\n",
    "    \n",
    "    \n",
    "As an example of the challenge of learning `XArray`: The reduction of this data to binned profiles\n",
    "requires a non-trivial workflow. A naive approach can result in a calculation that should take \n",
    "a seconds run for hours. (A key idea of this workflow -- the sortby() step -- is found on page 137 of **PDSH**.)\n",
    "    \n",
    "    \n",
    "- `swap_dims()` to substitute `pressure` for `time` as the ordinate dimension\n",
    "- `sortby()` to make the `pressure` dimension monotonic\n",
    "- Create a pressure-bin array to guide the subsequent data reduction\n",
    "- `groupby_bins()` together with `mean()` to reduce the data to a 0.25 meter quantized profile\n",
    "- use `transpose()` to re-order wavelength and pressure, making the resulting `DataArray` simpler to plot\n",
    "- accumulate these results by day as a list of `DataArrays`\n",
    "- From this list create an `XArray Dataset`\n",
    "- Write this to a new NetCDF file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Example: XArray Dataset subset and chart\n",
    "\n",
    "Time dimension slice:\n",
    "\n",
    "```\n",
    "ds = xr.open_dataset(\"file.nc\")\n",
    "ds = ds.sel(time=slice(t0, t1))\n",
    "ds\n",
    "```\n",
    "\n",
    "This shows that the temperature Data Variable has a cumbersome name: \n",
    "`sea_water_temperature_profiler_depth_enabled`. \n",
    "\n",
    "```\n",
    "ds = ds.rename({'sea_water_temperature_profiler_depth_enabled':'temperature'})\n",
    "```\n",
    "\n",
    "Plot this against the default dimension `time`:\n",
    "\n",
    "```\n",
    "ds.temperature.plot()\n",
    "```\n",
    "\n",
    "Temperature versus depth rather than time:\n",
    "\n",
    "```\n",
    "fig, axs = plt.subplots(figsize=(12,4), tight_layout=True)\n",
    "axs.plot(ds.temperature, -ds.z, marker='.', markersize=9., color='k', markerfacecolor='r')\n",
    "axs.set(ylim = (200., 0.), title='temperature against depth')\n",
    "```\n",
    "\n",
    "Here `ds.z` is negated to indicate depth below ocean surface.\n",
    "\n",
    "\n",
    "### More cleanup of Datasets: rename() and drop()\n",
    "\n",
    "* Use `ds = ds.rename(dictionary-of-from-to)` to rename data variables in a Dataset\n",
    "* Use `ds = ds.drop(string-name-of-data-var)` to get rid of a data variable\n",
    "* Use `ds = ds[[var1, var2]]` to eliminate all but those two variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-blackjack",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "### XArray ***DataArray*** name and length\n",
    "\n",
    "\n",
    "```\n",
    "sensor_t.name\n",
    "\n",
    "len(sensor_t)\n",
    "len(sensor_t.time)           # gives same result\n",
    "```\n",
    "\n",
    "What is the name of the controlling dimension?\n",
    "\n",
    "```\n",
    "if sensor_t.dims[0] == 'time': print('time is dimension zero')\n",
    "```\n",
    "\n",
    "Equivalent; but the second version permits reference by \"discoverable\" string.\n",
    "\n",
    "\n",
    "```\n",
    "sensor_t = ds_CTD_time_slice.seawater_temperature\n",
    "sensor_t = ds_CTD_time_slice['seawater_temperature']\n",
    "```\n",
    "\n",
    "#### Plotting with scaling and offsetting\n",
    "\n",
    "Suppose I wish to shift some data left to contrast it with some other data (where they would clobber one another)...\n",
    "\n",
    "```\n",
    "sensor_t + 0.4\n",
    "```\n",
    "\n",
    "Suppose I wish to scale some data in a chart to make it easier to interpret given a fixed axis range\n",
    "\n",
    "```\n",
    "sensor_t * 10.               # this fails by trying to make ten copies of the array\n",
    "\n",
    "np.ones(71)*3.*smooth_t      # this works by creating an inner product\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-series",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "\n",
    "## Pandas Series and DataFrames\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "### Summary\n",
    "\n",
    "### DataFrames\n",
    "\n",
    "DataFrames:\n",
    "\n",
    "* constructor takes `data=<ndarray>` and both `index` and `columns` arguments... \n",
    "    * ...2 dimensions only: higher dimensions and they say 'use XArray'\n",
    "    * ...and switching required a `.T` transpose\n",
    "* indexing by column and row header values, separated as in `[column_header][row_header]`\n",
    "    * as this reverses order from ndarrays: Better confirm... seems to be the case\n",
    "    * skip index/columns: defaults to integers.\n",
    " \n",
    "Below this section we go into n-dimensional arrays in Numpy, the *ndarray*. Here we take this \n",
    "for granted and look at the relationship with DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mediterranean-schedule",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ndarray from a list of lists (notice no comma delimiter):\n",
      "\n",
      " [['l' 'i' 's' 't' '1']\n",
      " ['s' 'c' 'n' 'd' '2']\n",
      " ['t' 'h' 'r' 'd' '3']] \n",
      "\n",
      "and indexing comparison: first [0][2] then [2][0]: s t\n",
      "\n",
      "and tuplesque indexing [0, 2] or [2, 0] equivalently gives: s t\n",
      "\n",
      "So ndarrays index [slow][fast] equivalent to [row][column]\n",
      "\n",
      "\n",
      "Moving on to DataFrames:\n",
      "\n",
      "\n",
      "     col_a col_b col_c col_d col_e\n",
      "2row     l     i     s     t     1\n",
      "4row     s     c     n     d     2\n",
      "6row     t     h     r     d     3 \n",
      "\n",
      "is a DataFrame from the ndarray; so now index [\"col_c\"][\"6row\"]: r\n",
      "\n",
      "Here is a Dataframe from a transpose of the ndarray\n",
      "\n",
      "       2row 4row 6row\n",
      "col_a    l    s    t\n",
      "col_b    i    c    h\n",
      "col_c    s    n    r\n",
      "col_d    t    d    d\n",
      "col_e    1    2    3 \n",
      "\n",
      "indexing 2row then col_e: 1\n",
      "\n",
      "So the column of a DataFrame is indexed first, then the row: Reverses the sense of the 2D ndarray.\n",
      "\n",
      "Now skipping the \"index=\"\" argument so the row labels default to integers:\n",
      "\n",
      "  col_a col_b col_c col_d col_e\n",
      "0     l     i     s     t     1\n",
      "1     s     c     n     d     2\n",
      "2     t     h     r     d     3 \n",
      "\n",
      "...so now indexing [\"col_d\"][0]: t \n",
      "\n",
      "      0  1  2  3  4\n",
      "2row  l  i  s  t  1\n",
      "4row  s  c  n  d  2\n",
      "6row  t  h  r  d  3 \n",
      "\n",
      "having done it the other way: used index= but not columns=. Here is element [0][\"4row\"]: s\n",
      "\n",
      "\n",
      "Starting from an XArray Dataset and using .to_dataframe() we arrive at a 2D structure.\n",
      "\n",
      "For example: df = ds_CTD.seawater_pressure.to_dataframe()\n",
      " \n",
      "The problem is that the resulting dataframe may not be indexed (row sense) using integers. A fix\n",
      "is necessary to override the index and columns attributes of the dataframe, as in:\n",
      " \n",
      "             df.index=range(len(df))\n",
      "             df.columns=range(1)\n",
      " \n",
      "results in a dataframe that one can index with integers [0] for column first then [n] for row.\n",
      "This example came from the profile time series analysis to get ascent start times and so on.\n",
      "The problem is it is a case of too much machinery. It is far simpler to use a pandas Series.\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "#\n",
    "# A micro study of ndarray to DataFrame translation\n",
    "#\n",
    "###################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Here is an ndarray construction from a built list of lists (not used in what follows): \n",
    "# arr = np.array([range(i, i+5) for i in [2, 4, 6]])                                       \n",
    "#     ... where the range() runs across columns; 2 4 6 are rows\n",
    "\n",
    "# ndarray construction: Notice all list elements are of the same type (strings)\n",
    "arr = np.array([['l','i','s','t','1'],['s','c','n','d','2'],['t','h','r','d', '3']])\n",
    "\n",
    "print('\\nndarray from a list of lists (notice no comma delimiter):\\n\\n', arr, \\\n",
    "      '\\n\\nand indexing comparison: first [0][2] then [2][0]:', arr[0][2], arr[2][0]) \n",
    "print('\\nand tuplesque indexing [0, 2] or [2, 0] equivalently gives:', arr[0,2], arr[2,0])\n",
    "print('\\nSo ndarrays index [slow][fast] equivalent to [row][column]\\n\\n\\nMoving on to DataFrames:\\n\\n')\n",
    "\n",
    "rowlist=[\"2row\", \"4row\", \"6row\"]\n",
    "columnlist = [\"col_a\", \"col_b\", \"col_c\", \"col_d\", \"col_e\"]\n",
    "df = pd.DataFrame(data=arr, index=rowlist, columns=columnlist)\n",
    "\n",
    "print(df, '\\n\\nis a DataFrame from the ndarray; so now index [\"col_c\"][\"6row\"]:', df['col_c']['6row'])\n",
    "\n",
    "df = pd.DataFrame(data=arr.T, index=columnlist, columns=rowlist)\n",
    "\n",
    "print('\\nHere is a Dataframe from a transpose of the ndarray\\n\\n', df, \\\n",
    "      '\\n\\nindexing 2row then col_e:', df['2row']['col_e'])\n",
    "print('\\nSo the column of a DataFrame is indexed first, then the row: Reverses the sense of the 2D ndarray.\\n')\n",
    "print('Now skipping the \"index=\"\" argument so the row labels default to integers:\\n')\n",
    "\n",
    "df = pd.DataFrame(data=arr, columns=columnlist)\n",
    "\n",
    "print(df, '\\n\\n...so now indexing [\"col_d\"][0]:', df['col_d'][0], '\\n')\n",
    "\n",
    "df = pd.DataFrame(data=arr, index=rowlist)\n",
    "\n",
    "print(df, '\\n\\nhaving done it the other way: used index= but not columns=. Here is element [0][\"4row\"]:', \\\n",
    "      df[0]['4row'])\n",
    "\n",
    "\n",
    "print('\\n\\nStarting from an XArray Dataset and using .to_dataframe() we arrive at a 2D structure.\\n')\n",
    "print('For example: df = ds_CTD.seawater_pressure.to_dataframe()')\n",
    "print(' ')\n",
    "print('The problem is that the resulting dataframe may not be indexed (row sense) using integers. A fix')\n",
    "print('is necessary to override the index and columns attributes of the dataframe, as in:')\n",
    "print(' ')\n",
    "print('             df.index=range(len(df))')\n",
    "print('             df.columns=range(1)')\n",
    "print(' ')\n",
    "print('results in a dataframe that one can index with integers [0] for column first then [n] for row.')\n",
    "print('This example came from the profile time series analysis to get ascent start times and so on.')\n",
    "print('The problem is it is a case of too much machinery. It is far simpler to use a pandas Series.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d6a6de",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "### Selecting based on a range\n",
    "\n",
    "\n",
    "Suppose we have a DataFrame with a column of timestamps over a broad time range and we would like to focus on only a subset. \n",
    "One approach would be to generate a smaller dataframe that meets the small time criterion and iterate over that.\n",
    "\n",
    "The following cell builds a pandas DataFrame with a date column; then creates a subset DataFrame where only rows in\n",
    "a time range are preserved. This is done twice: First using conditional logic and then using the same with '.loc'. \n",
    "('loc' and 'iloc' are location-based indexing, the first relying on labels and the second on integer position.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a58cc95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[numpy.datetime64('2020-10-11') 7 13 6]\n",
      " [numpy.datetime64('2020-10-12') 7 9 6]\n",
      " [numpy.datetime64('2020-10-13') 7 8 6]\n",
      " [numpy.datetime64('2020-10-14') 7 5 6]\n",
      " [numpy.datetime64('2020-10-15') 7 11 6]]\n",
      "\n",
      "arr[0][2] then [2][0]: 13 2020-10-13\n",
      "\n",
      "and tuplesque indexing [0, 2] or [2, 0] equivalently gives: 13 2020-10-13\n",
      "\n",
      "using conditionals:\n",
      "\n",
      "            date data1 data2 data3\n",
      "day3 2020-10-13     7     8     6\n",
      "day4 2020-10-14     7     5     6 \n",
      "\n",
      "\n",
      "using loc:\n",
      "\n",
      "            date data1 data2 data3\n",
      "day3 2020-10-13     7     8     6\n",
      "day4 2020-10-14     7     5     6\n",
      "\n",
      "notice the results are identical; so it is an open question \"Why use `loc`?\"\n"
     ]
    }
   ],
   "source": [
    "from numpy import datetime64 as dt64, timedelta64 as td64\n",
    "\n",
    "t0=dt64('2020-10-11')\n",
    "t1=dt64('2020-10-12')\n",
    "t2=dt64('2020-10-13')\n",
    "t3=dt64('2020-10-14')\n",
    "t4=dt64('2020-10-15')\n",
    "\n",
    "r0 = dt64('2020-10-12')\n",
    "r1 = dt64('2020-10-15')\n",
    "\n",
    "arr = np.array([[t0,7,13,6],[t1,7,9,6],[t2,7,8,6],[t3,7,5,6],[t4,7,11,6]])\n",
    "\n",
    "print(arr)\n",
    "print('\\narr[0][2] then [2][0]:', arr[0][2], arr[2][0]) \n",
    "print('\\nand tuplesque indexing [0, 2] or [2, 0] equivalently gives:', arr[0,2], arr[2,0])\n",
    "\n",
    "rowlist    = [\"day1\", \"day2\",\"day3\",\"day4\",\"day5\"]\n",
    "columnlist = [\"date\", \"data1\", \"data2\", \"data3\"]\n",
    "df = pd.DataFrame(data=arr, index=rowlist, columns=columnlist)\n",
    "\n",
    "\n",
    "df_conditional = df[(df['date'] > r0) & (df['date'] < r1)]\n",
    "print('\\nusing conditionals:\\n\\n', df_conditional, '\\n')\n",
    "\n",
    "\n",
    "df_loc = df.loc[(df['date'] > r0) & (df['date'] < r1)]\n",
    "print('\\nusing loc:\\n\\n', df_loc)\n",
    "\n",
    "print('\\nnotice the results are identical; so it is an open question \"Why use `loc`?\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-following",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "## Numpy ndarrays\n",
    "\n",
    "\n",
    "* do not have row and column headers; whereas pandas DataFrames do have typed headers\n",
    "* indexing has an equivalence of `[2][0]` to `[2,0]` \n",
    "    * The latter (with comma) is the presented way in PDSH\n",
    "    * This duality does not work for DataFrames\n",
    "* has row-then-column index order...\n",
    "    * ....with three rows in `[['l','i','s','t','1'],['s','c','n','d','2'],['t','h','r','d','3']]` \n",
    "* has slice by dimension as `start:stop:step` by default `0, len (this dimension), 1` \n",
    "    * ...exception: when `step` is negative `start` and `stop` are reversed\n",
    "    * ...multi-dimensional slices separated by commas\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-trade",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "\n",
    "## Time\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "There is time in association with data (when a sample was recorded) and time in association with\n",
    "code development (how long did this cell take to run?) Let's look at both.\n",
    "\n",
    "\n",
    "### Sample timing\n",
    "\n",
    "See PDSH-189. There are two time mechanisms in play: Python's built-in `datetime` and an improvement called\n",
    "`datetime64` from **numpy** that enables *arrays* of dates, i.e. time series. \n",
    "\n",
    "\n",
    "Consider these two ways of stipulating time slice arguments for `.sel()` applied to a DataSet.\n",
    "First:  Use a datetime64 with precision to minutes (or finer).\n",
    "Second: Pass strings that are interpreted as days, inclusive. In pseudo-code: \n",
    "\n",
    "```\n",
    "if do_precision:  \n",
    "   t0 = dt64('2019-06-01T00:00')\n",
    "   t1 = dt64('2019-06-01T05:20')\n",
    "   dss = ds.sel(time=slice(t0, t1))   \n",
    "else:\n",
    "    day1 = '24'\n",
    "    day2 = '27'              # will be 'day 27 inclusive' giving four days of results\n",
    "    dss = ds.sel(time=slice('2019-06-' + day1, '2019-08-' + day2))\n",
    "\n",
    "len(dss.time)\n",
    "```\n",
    "\n",
    "### Execution timing\n",
    "\n",
    "Time of execution in seconds: \n",
    "\n",
    "```\n",
    "from time import time\n",
    "\n",
    "toc = time()\n",
    "for i in range(12): j = i + 1\n",
    "tic = time()\n",
    "print(tic - toc)\n",
    "\n",
    "7.82012939453125e-05\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-sellers",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "## ipywidgets\n",
    "\n",
    "\n",
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-toolbox",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "\n",
    "## Holoview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-wesley",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "## Instruments\n",
    "\n",
    "\n",
    "### Specifically spectrophotometer (SP) and Nitrate\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "\n",
    "The SP runs on ascent only, at about 3.7 samples per second. Compare nitrate that also runs \n",
    "on ascent only at about 3 samples per minute. Nitrate data is fairly straightforward; SP \n",
    "data is chaotic/messy. The objective is to reduce the SP to something interpretable.\n",
    "\n",
    "\n",
    "### Deconstructing data: process pattern\n",
    "\n",
    "\n",
    "- `ds = xr.open_dataset(fnm)` \n",
    "    - Data dispersed across files: Variant + wildcard: `ds = xr.open_mfdataset('data_with_*_.nc')`\n",
    "- `obs` dimensional coordinate creates degeneracy over multiple files\n",
    "    - Use `.swap_dims` to swap time for `obs`\n",
    "- `ds.time[0].values, ds.time[-1].values` gives a timespan but nothing about duty cycles\n",
    "    - 2019 spectrophotometer data at Oregon Slope Base: 86 channels, 7 million samples\n",
    "    - ...leading to...\n",
    "        - Only operates during midnight and noon ascent; at 3.7 samples per second\n",
    "        - Optical absorbance and beam attenuation are the two data types\n",
    "        - Data has frequent dropouts over calendar time\n",
    "        - Data has spikes that tend to register across all 86 channels\n",
    "        - Very poor documentation; even the SME report is cursory\n",
    "\n",
    "\n",
    "    \n",
    "### Nitrate \n",
    "\n",
    "    \n",
    "This code follows suit the spectrophotometer. It is simpler because there is only a nitrate value \n",
    "and no wavelength channel. \n",
    "\n",
    "    \n",
    "I kept the pressure bins the same even though the nitrate averates about 3 three samples or less per minute\n",
    "during a 70 minute ascent. That's about three meters per minute so one sample per meter. Since the \n",
    "spectrophotometer bin depth is 0.25 meters there are necessarily a lot of empty bins (bins with no data)\n",
    "for the nitrate profile. \n",
    "\n",
    "    \n",
    "### Two open issues\n",
    "\n",
    "\n",
    "A curious artifact of the situation is from a past bias: I had understood that the SCIP makes pauses \n",
    "on descent to accommodate the nitrate sensor. I may be in error but now it looks like this sensor, \n",
    "the nitrate sensor, is observing on ascent which is continuous. This leaves open the question of \n",
    "why the pauses occur on the descent. If I have that right. \n",
    "\n",
    "\n",
    "Finally there are two nitrate signals: 'samp' and 'dark'. This code addresses only 'samp' as 'dark'\n",
    "is showing nothing of interest. So this is an open issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "super-pocket",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML, Video   \u001b[38;5;66;03m# HTML is ...?...\u001b[39;00m\n\u001b[1;32m     30\u001b[0m                                           \u001b[38;5;66;03m# Video is used for load/playback\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# First set up the figure, the axis, and the plot element we want to animate\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots()\n\u001b[1;32m     35\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_xlim(( \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     36\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Animation in Python is one thing. Animation in a Jupyter notebook is another.\n",
    "# Animation in binder is yet another. Rather than try and bootstrap a lesson here\n",
    "# I present a sequence of annotated steps that create an animation, save it as \n",
    "# an .mp4 file, load it and run it: In a Jupyter notebook of course. Then we\n",
    "# will see how it does in binder.\n",
    "\n",
    "# At some point in working on this I did a conda install ffmpeg. I am not clear \n",
    "#   right now on whether this was necessary or not; I suspect not.\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# With [the inline] backend activated with this line magic matplotlib command, the output \n",
    "# of plotting commands is displayed inline within frontends like the Jupyter notebook, \n",
    "# directly below the code cell that produced it. The resulting plots will then also be stored \n",
    "# in the notebook document.\n",
    "\n",
    "# de rigeur, commented out here as this runs at the top of the notebook\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import animation, rc      # animation provides tools to build chart-based animations.\n",
    "                                          # Each time Matplotlib loads, it defines a runtime configuration (rc) \n",
    "                                          #   containing the default styles for every plot element you create. \n",
    "                                          #   This configuration can be adjusted at any time using \n",
    "                                          #   the plt. ... matplotlibrc file, which you can read about \n",
    "                                          #   in the Matplotlib documentation.\n",
    "\n",
    "\n",
    "from IPython.display import HTML, Video   # HTML is ...?...\n",
    "                                          # Video is used for load/playback\n",
    "\n",
    "# First set up the figure, the axis, and the plot element we want to animate\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlim(( 0, 2))\n",
    "ax.set_ylim((-2, 2))\n",
    "\n",
    "line, = ax.plot([], [], lw=2)\n",
    "\n",
    "# initialization function: plot the background of each frame\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    return (line,)\n",
    "\n",
    "# animation function. This is called sequentially\n",
    "def animate(i):\n",
    "    x = np.linspace(0, 2, 1000)\n",
    "    y = np.sin(2 * np.pi * (x - 0.01 * i))\n",
    "    line.set_data(x, y)\n",
    "    return (line,)\n",
    "\n",
    "# call the animator. blit=True means only re-draw the parts that have changed.\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=100, interval=12, blit=True)\n",
    "\n",
    "HTML(anim.to_html5_video())\n",
    "\n",
    "# print(anim._repr_html_() is None) will be True\n",
    "# anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_line(num, data, line):\n",
    "    line.set_data(data[..., :num])\n",
    "    return line,\n",
    "\n",
    "fig1 = plt.figure()\n",
    "\n",
    "data = .05 + 0.9*np.random.rand(2, 200)\n",
    "l, = plt.plot([], [], 'r-')                # l, takes the 1-tuple returned by plt.plot() and grabs that first \n",
    "                                           # and only element; so it de-tuples it\n",
    "\n",
    "plt.xlim(0, 1); plt.ylim(0, 1); plt.xlabel('x'); plt.title('test')\n",
    "\n",
    "lines_anim = animation.FuncAnimation(fig1, update_line, 200, fargs=(data, l), interval=1, blit=True)\n",
    "\n",
    "# fargs are additional arguments to 'update_line()' in addition to the frame number: data and line\n",
    "# interval is a time gap between frames (guess is milliseconds)\n",
    "# blit is the idea of modifying only pixels that change from one frame to the next\n",
    "\n",
    "# For direct display use this: HTML(line_ani.to_html5_video())\n",
    "lines_anim.save('./lines_tmp3.mp4')            # save the animation to a file\n",
    "Video(\"./lines_tmp3.mp4\")                      # One can add , embed=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure()\n",
    "\n",
    "x = np.arange(-9, 10)\n",
    "y = np.arange(-9, 10).reshape(-1, 1)\n",
    "base = np.hypot(x, y)\n",
    "ims = []\n",
    "for add in np.arange(15):\n",
    "    ims.append((plt.pcolor(x, y, base + add, norm=plt.Normalize(0, 30)),))\n",
    "\n",
    "im_ani = animation.ArtistAnimation(fig2, ims, interval=50, repeat_delay=3000,\n",
    "                                   blit=True)\n",
    "# To save this second animation with some metadata, use the following command:\n",
    "# im_ani.save('im.mp4', metadata={'artist':'Guido'})\n",
    "\n",
    "HTML(im_ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-client",
   "metadata": {},
   "source": [
    "## Binder\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "* Create a binder badge in the home page `README.md` of the repository. \n",
    "\n",
    "```\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/<accountname>/<reponame>/HEAD)\n",
    "\n",
    "```\n",
    "\n",
    "* In `<repo>/binder` create `environment.yml` to match the working environment\n",
    "    * For this repo as of 10/23/2021 `binder/environment.yml` was: \n",
    "\n",
    "\n",
    "```\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - matplotlib\n",
    "  - netcdf4\n",
    "  - xarray\n",
    "  - ffmpeg\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

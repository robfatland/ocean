{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f04978-c60b-42ca-8688-4fb4944bdfbc",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "A number of technical elements went into creating this Jupyter notebok repository. \n",
    "Many of them are described here and in the companion **Technical Guide** notebook.\n",
    "\n",
    "\n",
    "This notebook refers to Jake VanDerplas' excellent (and freely available)\n",
    "[Python Data Science Handbook]((https://jakevdp.github.io/PythonDataScienceHandbook/), \n",
    "abbreviated here PDSH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a5afe5e-0f5b-45f7-a6bf-bb4b7b728910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>table {float:left}</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>table {float:left}</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b81de1-0473-4b10-8ce1-8b67388bf480",
   "metadata": {},
   "source": [
    "## [Table-of-Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "Eventually this notebook will be parsed into several.\n",
    "\n",
    "\n",
    "* [Markdown](#Markdown)\n",
    "    * [LaTeX math formulas](#LaTeX-math-formulas)\n",
    "    * [Tables](#Tables)\n",
    "* [Reducing Datasets](#Reducing-Datasets)\n",
    "    * [One minute resampling](#One-minute-resampling)\n",
    "* [Plotting](#Plotting)\n",
    "    * [Making animations](#Making-animations)\n",
    "* [Multimedia](#Multimedia)\n",
    "    * [Images](#Images)\n",
    "    * [Animations](#Animations)\n",
    "    * [YouTube video playback](#YouTube-video-playback)\n",
    "    * [Sound clips](#Sound-clips)\n",
    "* [XArray Datasets and DataArrays](#XArray-Datasets-and-DataArrays)\n",
    "* [Pandas Series and DataFrames](#Pandas-Series-and-DataFrames)\n",
    "    * [Selecting based on a range](#Selecting-based-on-a-range)\n",
    "* [Numpy ndarrays](#Numpy-ndarrays)\n",
    "* [Time](#Time)\n",
    "* [ipywidgets](#ipywidgets)\n",
    "* [HoloView](#Holoview)\n",
    "* [Where I Left Off](#process-these-notes)\n",
    "* [Binder](#Binder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aadd23-0925-4846-8251-a55099e9c485",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "## Markdown\n",
    "\n",
    "Markdown cells support HTML (particularly images)\n",
    "\n",
    "\n",
    "<BR>\n",
    "<img src=\"../Images/people/dubious.png\" style=\"float: left;\" alt=\"drawing\" width=\"120\"/>\n",
    "<div style=\"clear: left\"><BR><BR>\n",
    "\n",
    "\n",
    "|tables|like\n",
    "|--|--\n",
    "|this|one\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7db3424-5874-4553-ad15-8c8ff677a5f7",
   "metadata": {},
   "source": [
    "$LaTeX^{\\pi}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2582a626-bfa7-4f60-aced-2e519ff5d6dc",
   "metadata": {},
   "source": [
    "- bullet\n",
    "- lists\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "'blocks of code...'\n",
    "# with fixed font width\n",
    "```\n",
    "\n",
    "> \"quotation-style\" text\n",
    "\n",
    "\n",
    "and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b23edc3-886b-41e7-8189-3882c1723667",
   "metadata": {},
   "source": [
    "### LaTeX math formulas \n",
    "\n",
    "\n",
    "Place formulas inline via single dollar-sign delimiters: `$e^x = \\sum_{i=0}^{\\infty}{\\frac{x^i}{i!}}$` looks like this: $e^x = \\sum_{i=0}^{\\infty}{\\frac{x^i}{i!}}$. Double dollar-sign delimiters create a centered \n",
    "equation, not inline. \n",
    "\n",
    "\n",
    "$$e^x = \\sum_{i=0}^{\\infty}{\\frac{x^i}{i!}}$$\n",
    "\n",
    "\n",
    "Single-dollar-sign, `\\Large` format:\n",
    "\n",
    "\n",
    "$\\Large{e^x = \\sum_{i=0}^{\\infty}{\\frac{x^i}{i!}}}$\n",
    "\n",
    "\n",
    "LaTeX documentation is mostly followed in Jupyter notebooks with a few differences. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc572f-07e7-4aa5-bf57-180b98540873",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "\n",
    "### Tables\n",
    "\n",
    "\n",
    "To left-justify tables: Create and run a Python cell at the top of the\n",
    "notebook with just these contents (uses cell magic): \n",
    "\n",
    "```\n",
    "%%html\n",
    "<style>table {float:left}</style>\n",
    "```\n",
    "\n",
    "Pipes `|`, colons `:` and hyphens `-` can be used to create tables. Colons are used to \n",
    "imply left/right/center alignment.\n",
    "\n",
    "\n",
    "| Streichduo | PDF | SHO $\\nu$ (khz) | What? | Ok |\n",
    "| :- | -: | :-: | :-: | :-\n",
    "| Bach | Eulerian | 14. | Why?\n",
    "| Mozart | Gaussian | 17.922 | | this column is distended to accommodate a long remark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae0fd4-1bb0-4386-a831-0070aac487b6",
   "metadata": {},
   "source": [
    "The left-align cell creates a *different* issue: Subsequent markdown is mis-formatted.\n",
    "Deal with this using a cell break.\n",
    "\n",
    "\n",
    "\n",
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "\n",
    "## Reducing Datasets\n",
    "\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "\n",
    "As a good reference for manipulating data see Jake VanDerplas' book\n",
    "Python Data Science Handbook. \n",
    "\n",
    "\n",
    "We have here multi-dimensional oceanography datasets bundled as\n",
    "NetCDF or CSV files. Corresponding Python modules are `XArray` and `pandas`.\n",
    "On import these are abbreviated `xr` and `pd` respectively.\n",
    "\n",
    "\n",
    "XArray has a method `.open_dataset('file.nc')` returning an XArray Dataset. \n",
    "A Dataset is a collection of XArray DataArray structures. The important structural\n",
    "feature of a Dataset is that it includes four sections: `Dimensions`, \n",
    "`Coordinates`, `Data Variables`, and `Attributes`. To examine a Dataset\n",
    "called `A`: Create an empty cell, type `A`, and run it. This will give a \n",
    "breakdown of `A` in terms of these four constituent sections. \n",
    "\n",
    "\n",
    "In pandas the data structure of interest is a DataFrame. DataFrames are used \n",
    "extensively in these notebooks to manage profile metadata for shallowo profilers.\n",
    "For more on this see the accompanying Technical Guide. \n",
    "\n",
    "\n",
    "Common reductive steps once data are read include removing extraneous components from\n",
    "a dataset, downsampling, removing NaN values, changing the primary `dimension`\n",
    "from `obs` (for 'observation') to `time`, combining multiple data files into \n",
    "a single dataset, saving modified datasets to new files, and creating data charts. \n",
    "\n",
    "\n",
    "One reason datasets are reduced in size is to enable them to reside within this\n",
    "[GitHub repository](https://github.com/robfatland/ocean)\n",
    "without exceeding memory size limits. If large datasets are still\n",
    "needed, they can be downloaded by a notebook. This is a further means of reducing\n",
    "repo size. An example of this download-as-needed using `wget`\n",
    "is in the **`Global Ocean`** notebook.\n",
    "The five data files downloaded used in that notebook are each 17MB, reduced \n",
    "from 100MB by the following Python code which retains only data of interest.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# Reduce volume of an XArray Dataset with extraneous Data Variables:\n",
    "T=xr.open_dataset('glodap_oxygen.nc')\n",
    "T.nbytes\n",
    "T=T[['temperature', 'Depth']]\n",
    "T.nbytes\n",
    "T.to_netcdf('temperature.nc')      \n",
    "```\n",
    "\n",
    "\n",
    "Specific to the RCA shallow profilers: Data collected during ascent spans 200 meters\n",
    "of water column depth in one hour, about 6 cm / sec. To perceive\n",
    "a 'thin layer' signal suggests starting from high rate data (example: one sample\n",
    "/ sec) and averaging -- if at all -- to a guess at minimum layer thickness.\n",
    "More in the spirit of XArray would be to resample based on depth bins.\n",
    "\n",
    "\n",
    "As an aside, one of the challenges of working with complex data formats such as\n",
    "XArray Datasets is learning how to access data subsets. While this repository \n",
    "is not a comprehensive guide it does include a number of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a5517b-4edf-453f-bf3e-94005d3fb05f",
   "metadata": {},
   "source": [
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "### Oregon Slope Base shallow profiler sensor abbreviation table \n",
    "\n",
    "\n",
    "| abbrev | sensors |remarks|\n",
    "|--|--|--|\n",
    "|ctdpf|3|CTD: includes salinity, temperature, dissolved oxygen\n",
    "|pco2|1|carbonate chemistry, midnight and noon *descent* only\n",
    "|phsen|1|pH, midnight and noon *descent* only\n",
    "|nutnr|2|nitrate, dark samples; midnight and noon *ascent* only\n",
    "|flort|3|Fluorometer triplet: chlorophyll-A, FDOM, backscatter ('bb700')\n",
    "|spkir|7|downwelling spectral irradiance, 7 frequency bands\n",
    "|parad|1|photosynthetically available radiation 'PAR'\n",
    "|optaa|86|spectrophotometer: 86 frequency bands, ascent local noon and midnight only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-rabbit",
   "metadata": {},
   "source": [
    "### Slow resampling problem\n",
    "\n",
    "\n",
    "The shallow profiler spectrophotometer has 86 channels. Each observation has \n",
    "a corresponding depth and time, typically several thousand per profile.\n",
    "The XArray Dataset has `time` swapped in for `obs` dimension but we are \n",
    "interested in resampling into depth bins. This took hours; which was \n",
    "puzzling. However page 137 of PDSH, on **Rearranging Multi-Indices**\n",
    "and **Sorted and unsorted indices** provides this resolution: \n",
    "\n",
    "> ***Rearranging Multi-indices***<BR>\n",
    "One of the keys to working with multiply indexed data is knowing how to effectively \n",
    "transform the data. There are a number of operations that will preserve all the \n",
    "information in the dataset, but rearrange it for the purposes of various computations. \n",
    "[...] There are many [ways] to finely control the rearrangement\n",
    "of data between heirarchical indices and columns.\n",
    "    \n",
    "> ***Sorted and unsorted indices***<BR>\n",
    "Earlier, we briefly mentioned a caveat, but we should emphasize it more here. \n",
    "*Many of the `MultiIndex`slicing operations will fail if the index is not sorted.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-retro",
   "metadata": {},
   "source": [
    "### One minute resampling\n",
    "\n",
    "#### [Top](#Introduction) and [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "\n",
    "`XArray Datasets` feature selection by time range: `ds.sel(time=slice(timeA, timeB))`\n",
    "and resampling by time interval: `ds.resample(time='1Min').mean()`. \n",
    "(Substitute `.std()` to expand into standard deviation signals.)\n",
    "\n",
    "\n",
    "```\n",
    "ds = xr.open_dataset(ctd_data_filename)\n",
    "tJan1 = dt64('2019-01-01')\n",
    "tFeb1 = dt64('2019-02-01')\n",
    "ds = ds.sel(time=slice(tJan1, tFeb1))\n",
    "ds1Min = ds.resample(time='1Min').mean()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "The problem however is that the resample() execution in the code above\n",
    "can hang. The select operation `.sel()` is not understood by XArray as a monotonic\n",
    "time dimension monotonic. It may be treated as a jumble even if it is not! \n",
    "This can become even more catastrophic when other dimensions are present. \n",
    "The following work-around uses `pandas Dataframes`. \n",
    "\n",
    "\n",
    "\n",
    "This code moves the \n",
    "XArray Dataset contents into a pandas DataFrame.\n",
    "Here they are resampled properly; and the resulting\n",
    "columns are converted into a list of XArray DataArrays.\n",
    "These are then combined to form a new Dataset with \n",
    "the desired resampling completed quickly. \n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "df   = ds.to_dataframe().resample(\"1Min\").mean()\n",
    "vals = [xr.DataArray(data=df[c],                  \\\n",
    "                     dims=['time'],               \\\n",
    "                     coords={'time':df.index},    \\\n",
    "                     attrs=ds[c].attrs)           \\\n",
    "           for c in df.columns]\n",
    "ds = xr.Dataset(dict(zip(df.columns, vals)), attrs=ds.attrs)\n",
    "ds.to_netcdf('new_data_file.nc')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-boating",
   "metadata": {},
   "source": [
    "Flourometry code redux: For OSB shallow profiler triplet, to 1Min samples, JAN 2019\n",
    "\n",
    "\n",
    "```\n",
    "ds_Fluorometer = xr.open_dataset('/data/rca/fluorescence/osb_sp_flort_2019.nc')\n",
    "time_jan1, time_feb1 = dt64('2019-01-01'), dt64('2019-02-01')\n",
    "ds_Fluor_jan2019 = ds_Fluorometer.sel(time=slice(time_jan1, time_feb1))\n",
    "df               = ds_Fluor_jan2019.to_dataframe().resample(\"1Min\").mean()\n",
    "vals             = [xr.DataArray(data=df[c], dims=['time'], coords={'time':df.index}, \\\n",
    "                    attrs=ds_Fluor_jan2019[c].attrs) for c in df.columns]\n",
    "xr.Dataset(dict(zip(df.columns, vals)), \\\n",
    "           attrs=ds_Fluor_jan2019.attrs).to_netcdf('./data/rca/fluorescence/osb_sp_fluor_jan2019.nc')\n",
    "```\n",
    "\n",
    "Spectral irradiance stopgap version: Break out by spectrum (should be dimension of just one file).\n",
    "\n",
    "```\n",
    "spectral_irradiance_source = '/data/rca/irradiance/'\n",
    "spectral_irradiance_data = 'osb_sp_spkir_2019.nc'\n",
    "ds_spectral_irradiance = xr.open_dataset(spectral_irradiance_source + spectral_irradiance_data)\n",
    "ds_spectral_irradiance\n",
    "time_jan1, time_feb1 = dt64('2019-01-01'), dt64('2019-02-01')\n",
    "ds_Irr_jan2019 = ds_spectral_irradiance.sel(time=slice(time_jan1, time_feb1))\n",
    "df = [ds_Irr_jan2019.sel(spectra=s).to_dataframe().resample(\"1Min\").mean() for s in ds_Irr_jan2019.spectra]\n",
    "r = [xr.Dataset(dict(zip(q.columns, \n",
    "                         [xr.DataArray(data=q[c], dims=['time'], coords={'time':q.index}, \\\n",
    "                                       attrs=ds_Irr_jan2019[c].attrs) for c in q.columns] \\\n",
    "                    )   ), \n",
    "                attrs=ds_Irr_jan2019.attrs)\n",
    "    for q in df]\n",
    "for i in range(7): r[i].to_netcdf('./data/rca/irradiance/osb_sp_irr_spec' + str(i) + '.nc')\n",
    "```\n",
    "\n",
    "\n",
    "Spectral irradiance related skeleton code showing use of `.isel(spectra=3)`: \n",
    "\n",
    "\n",
    "```\n",
    "ds = ds_spkir.sel(time=slice(time0, time1))\n",
    "da_depth = ds.int_ctd_pressure.resample(time='1Min').mean()\n",
    "dsbar = ds.resample(time='1Min').mean()\n",
    "dsstd = ds.resample(time='1Min').std()\n",
    "dsbar.spkir_downwelling_vector.isel(spectra=3).plot()\n",
    "\n",
    "\n",
    "plot_base_dimension = 4\n",
    "indices = [0, 1, 2, 3, 4, 5, 6]\n",
    "n_indices = len(indices)\n",
    "da_si, da_st = [], []\n",
    "\n",
    "\n",
    "for idx in indices: \n",
    "    da_si.append(dsbar.spkir_downwelling_vector.isel(spectra=idx))\n",
    "    da_st.append(dsstd.spkir_downwelling_vector.isel(spectra=idx))\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(n_indices, 2, figsize=(4*plot_base_dimension, plot_base_dimension*n_indices), /\n",
    "           sharey=True, tight_layout=True)\n",
    "\n",
    "\n",
    "axs[0][0].scatter(da_si[0], da_depth, marker=',', s=1., color='k') \n",
    "axs[0][0].set(ylim = (200., 0.), xlim = (-.03, .03), title='spectral irradiance averaged')\n",
    "axs[0][1].scatter(da_st[0], da_depth, marker=',', s=1., color='r')\n",
    "axs[0][1].set(ylim = (200., 0.), xlim = (0., .002), title='standard deviation')\n",
    "\n",
    "\n",
    "for i in range(1, n_indices):\n",
    "    axs[i][0].scatter(da_si[i], da_depth, marker=',', s=1., color='k')\n",
    "    axs[i][0].set(ylim = (200., 0.), xlim = (-.03, .03))\n",
    "    axs[i][1].scatter(da_st[i], da_depth, marker=',', s=1., color='r')\n",
    "    axs[i][1].set(ylim = (200., 0.), xlim = (0., .002))\n",
    "```\n",
    "\n",
    "Code for PAR\n",
    "\n",
    "```\n",
    "par_source = '/data/rca/par/'\n",
    "par_data = 'osb_sp_parad_2019.nc'\n",
    "ds_par = xr.open_dataset(par_source + par_data)\n",
    "time_jan1 = dt64('2019-01-01')\n",
    "time_feb1 = dt64('2019-02-01')\n",
    "ds_par_jan2019 = ds_par.sel(time=slice(time_jan1, time_feb1))\n",
    "df   = ds_par_jan2019.to_dataframe().resample(\"1Min\").mean()\n",
    "vals = [xr.DataArray(data=df[c], dims=['time'], coords={'time':df.index}, attrs=ds_par_jan2019[c].attrs) for c in df.columns]\n",
    "ds_par_jan2019_1Min = xr.Dataset(dict(zip(df.columns, vals)), attrs=ds_par_jan2019.attrs)\n",
    "osb_par_nc_file = \"./data/rca/par/osb_sp_par_jan2019.nc\"\n",
    "ds_par_jan2019_1Min.to_netcdf(osb_par_nc_file)\n",
    "```\n",
    "\n",
    "PAR view: during shallow profiler rise/fall sequences\n",
    "\n",
    "```\n",
    "t0, t1 = '2019-07-17T13', '2019-07-18T05'\n",
    "t0, t1 = '2019-07-17T18:40', '2019-07-17T19:40'\n",
    "t0, t1 = '2019-07-17T21', '2019-07-17T23:00'        # These are the nitrate profiles\n",
    "t0, t1 = '2019-07-18T21', '2019-07-18T23:00'\n",
    "t0, t1 = '2019-07-19T21', '2019-07-19T23:00'\n",
    "t0, t1 = '2019-07-17T18:40', '2019-07-17T19:40'     # These are the profiles prior to nitrate\n",
    "t0, t1 = '2019-07-18T18:40', '2019-07-18T19:40'\n",
    "t0, t1 = '2019-07-19T18:40', '2019-07-19T19:40'\n",
    "da = ds_parad.sel(time=slice(t0, t1)).par_counts_output\n",
    "p=da.plot.line(marker='o', figsize = (14,8), markersize=1, yincrease = True)\n",
    "```\n",
    "\n",
    "Staged 'nitrate' profile compared with 'normal' profile\n",
    "\n",
    "```\n",
    "t0, t1 = '2019-07-19T20:30', '2019-07-19T23:50'               # USE THIS!! This is a good nitrate profile time bracket\n",
    "t0, t1 = '2019-07-19T18:40', '2019-07-19T19:40'\n",
    "da = ds_parad.sel(time=slice(t0, t1)).int_ctd_pressure\n",
    "p=da.plot.line(marker='o', figsize = (14,8), markersize=1, yincrease = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-moses",
   "metadata": {},
   "source": [
    "## Plotting \n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "### Summary\n",
    "\n",
    "Here I use [**`pyplot`**](https://plotly.com/python/), the Python graphing library usually imported as `plt`. \n",
    "Plotly is part of the matplotlib package. It is super confusing unless one has\n",
    "an extensive block of time to study it with care. In particular\n",
    "we need to be experts at \"drawing charts\" both with `.scatter` and `.plot`. Furthermore\n",
    "there is a quick-and-dirty `.plot` within XArray Datasets that can save time during\n",
    "development by giving a quick sanity check. In this section I will cover the basics \n",
    "of charting data in this context.\n",
    "\n",
    "### To investigate\n",
    "\n",
    "* Jake talks about Seaborn in PDSH; worth a look\n",
    "\n",
    "### Diving in\n",
    "\n",
    "### Sharing an axis\n",
    "\n",
    "\n",
    "Suppose a figure has an axis or a list of axes. These have a method `.twiny()` which creates\n",
    "a copy that can have its own x-axis stipulated. Same thing for `.twinx()`. This is demonstrated\n",
    "in the **Ocean 01 D Photic Zone Reduction** notebook. It risks cluttering the chart with the idea\n",
    "of condensing information.\n",
    "\n",
    "### Grid of charts\n",
    "\n",
    "This is example code for time-series data. It sets up a 3 x 3 grid of charts. These are matched to a 2-D set of\n",
    "axes (the 'a' variable) with both the scatter() and plot() constructs.\n",
    "\n",
    "```\n",
    "rn = range(9); rsi = range(7)\n",
    "\n",
    "p,a=plt.subplots(3, 3, figsize=(14,14))    # first 3 is vertical count, second 3 is horizontal count\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "a[0,0].plot(ctdF.time, ctdF.depth, color='r');                                  a[0,0].set(ylim=(200.,0.), title='Depth')\n",
    "a[0,1].plot(ctdF.time, ctdF.salinity, color='k');                               a[0,1].set(title='Salinity')\n",
    "a[0,2].plot(ctdF.time, ctdF.temperature, color='b');                            a[0,2].set(title='Temperature')\n",
    "a[1,0].plot(ctdF.time, ctdF.dissolved_oxygen, color='b');                       a[1,0].set(title='Dissolved Oxygen')\n",
    "a[1,1].scatter(phF.time.values, phF.ph_seawater.values, color='r');             a[1,1].set(title='pH')\n",
    "a[1,2].scatter(nitrateF.time.values, nitrateF.scn.values, color='k');           a[1,2].set(title='Nitrate')\n",
    "a[2,0].plot(parF.time, parF.par_counts_output, color='k');                      a[2,0].set(title='Photosynthetic Light')\n",
    "a[2,1].plot(fluorF.time, fluorF.fluorometric_chlorophyll_a, color='b');         a[2,1].set(title='Chlorophyll')\n",
    "a[2,2].plot(siF.time, siF.si0, color='r');                                      a[2,2].set(title='Spectral Irradiance')\n",
    "\n",
    "a[2,0].text(dt64('2017-08-21T07:30'), 155., 'local midnight', rotation=90, fontsize=15, color='blue', fontweight='bold')\n",
    "a[2,2].text(dt64('2017-08-21T07:30'), 4.25, 'local midnight', rotation=90, fontsize=15, color='blue', fontweight='bold')\n",
    "\n",
    "tFmt   = mdates.DateFormatter(\"%H\")                 # an extended format for strftime() is \"%d/%m/%y %H:%M\"\n",
    "t0, t1 = ctdF.time[0].values, ctdF.time[-1].values  # establish same time range for each chart\n",
    "tticks = [dt64('2017-08-21T06:00'), dt64('2017-08-21T12:00'), dt64('2017-08-21T18:00')]\n",
    "\n",
    "for i in rn: j, k = i//3, i%3; a[j, k].set(xlim=(t0, t1),xticks=tticks); a[j, k].xaxis.set_major_formatter(tFmt)\n",
    "print('')\n",
    "```\n",
    "\n",
    "\n",
    "Please note that Software Carpentry (Python) uses a post-facto approach to axes. \n",
    "In what follows there is implicit use of numpy 'collapse data along a particular\n",
    "dimension' using the `axis` keyword. So this is non-trivial code; but main point \n",
    "it shows adding axes to the figure.\n",
    "\n",
    "```\n",
    "fig = plt.figure(figsize=(10,3))\n",
    "\n",
    "axes1 = fig.add_subplot(1,3,1)\n",
    "axes2 = fig.add_subplot(1,3,2)\n",
    "axes3 = fig.add_subplot(1,3,3)\n",
    "\n",
    "avg_data = numpy.mean(data, axis=0)\n",
    "min_data = numpy.min(data, axis=0)\n",
    "max_data = numpy.max(data, axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-contributor",
   "metadata": {},
   "source": [
    "### Making animations\n",
    "\n",
    "[Top](#Introduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca32a7",
   "metadata": {},
   "source": [
    "This section was lifted from the BioOptics.ipynb notebook and simplified. It illustrates **overloading** a chart to \n",
    "show multiple sensor profiles evolving over time (frames). It also illustrates using markers along a line plot to\n",
    "emphasize observation spacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "qualified-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This code creates the animation; requires some time so it is commented out for now.\n",
    "# anim = animation.FuncAnimation(fig, AnimateChart, init_func=AnimateInit, \\\n",
    "#                                frames=nframes, interval=250, blit=True, repeat=False)\n",
    "#\n",
    "# Use 'HTML(anim.to_html5_video())'' for direct playback\n",
    "# anim.save(this_dir + '/Images/animations/multisensor_animation.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c230db14",
   "metadata": {},
   "source": [
    "## Binder-friendly playback\n",
    "\n",
    "\n",
    "The cell above creates an animation file that is stored within this repository. \n",
    "The cell below plays it back (for example in **binder**) to show multiple profile animations.\n",
    "Nitrate is intermittent, appearing as a sky-blue line in 2 of every 9\n",
    "frames. The remaining sensors are present in each frame.\n",
    "\n",
    "\n",
    "There animation begins March 1 2021 and proceeds at a rate of nine frames (profiles) per day.\n",
    "Change playback speed using the video settings control at lower right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "raised-romantic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video controls  >\n",
       " <source src=\"data:None;base64,/home/kilroy/ocean/Notebooks../Images/animations/multisensor_animation.mp4\" type=\"None\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binder-friendly playback\n",
    "from IPython.display import HTML, Video\n",
    "import os\n",
    "Video(os.getcwd() + '../Images/animations/multisensor_animation.mp4', embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-parker",
   "metadata": {},
   "source": [
    "## Multimedia\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "\n",
    "<BR>\n",
    "<img src=\".././Images/fauna/dubious.png\" style=\"float: left;\" alt=\"dubious person trying to eat kelp\" width=\"100\"/>\n",
    "<div style=\"clear: left\">\n",
    "<BR>\n",
    "\n",
    "This png file of a child dubiously eating kelp illustrates transparent pixels. \n",
    "There are lots of free/native apps to do this. \n",
    "    \n",
    "    \n",
    "### Summary\n",
    "\n",
    "We want to include (in order of importance) images, animations of data and sound clips. The above image,\n",
    "incidentally, includes transparent pixels: Done on a PC using Paint 3D, where the process is a bit convoluted. \n",
    "\n",
    "### Images\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "The cleanest presentation for static images in my experience so far is to use HTML in a \n",
    "markdown cell. It looks as follows where `<BR>` is a line break giving some spacing. Note\n",
    "the relative path.\n",
    "\n",
    "```\n",
    "<BR>\n",
    "<img src=\"./../Images/vessels/revelle.jpg\" style=\"float: left;\" alt=\"ship and iceberg photo\" width=\"900\"/>\n",
    "<div style=\"clear: left\">\n",
    "<BR>\n",
    "```\n",
    "\n",
    "It is possible to include images in Python cells using PIL but it was more of a chore.\n",
    "\n",
    "\n",
    "### Animations\n",
    "    \n",
    "    \n",
    "[Top](#Introduction)\n",
    "\n",
    "    \n",
    "Once an mp4 file is written to the file system playback is simple: Import `Video` from `IPython.display`\n",
    "and play the file back using `Video` setting the `embed` flag True.\n",
    "\n",
    "\n",
    "```\n",
    "from IPython.display import Video\n",
    "Video(\"./<some_animation>.mp4\", embed=True)\n",
    "```\n",
    "\n",
    "Alternative to `, embed=True`: Turn on the inline back-end using `%matplotlib inline` line magic\n",
    "\n",
    "    \n",
    "#### YouTube video playback\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "    \n",
    "```\n",
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('sjfsUzECqK0')\n",
    "```\n",
    "    \n",
    "### Sound clips\n",
    "\n",
    "    \n",
    "[Top](#Introduction)\n",
    "\n",
    "```\n",
    "from IPython.display import Audio\n",
    "Audio(\"<audiofile>.mp3\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-spectrum",
   "metadata": {},
   "source": [
    "## XArray Datasets and DataArrays\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "### Summary\n",
    "\n",
    "There are a million little details about working with XArray Datasets, DataArrays, numpy arrays, pandas DataFrames,\n",
    "pandas arrays... let's begin! The main idea is that a **DataArray** is an object containing, in the spirit of \n",
    "the game, one sort of data; and a **Dataset** is a collection of associated **DataArray**s. \n",
    "\n",
    "\n",
    "### XArray ***Dataset*** basics\n",
    "\n",
    "**Datasets** abbreviated `ds` have components { dimensions, coordinates, data variables, \n",
    "attributes }.\n",
    "\n",
    "\n",
    "A **DataArray** relates to a **`name`**; needs elaboration. \n",
    "\n",
    "\n",
    "```\n",
    "ds.variables\n",
    "\n",
    "ds.data_vars                                  # 'dict-like object'\n",
    "\n",
    "for dv in ds.data_vars: print(dv)\n",
    "    \n",
    "choice = 2\n",
    "this_data_var = list(ds.data_vars)[choice]\n",
    "print(this_data_var)\n",
    "\n",
    "ds.coords\n",
    "ds.dims\n",
    "ds.attrs\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### Load via `open_mfdataset()` with dimension swap from `obs` to `time`\n",
    "\n",
    "\n",
    "A single NetCDF (`.nc`) file can be opened as an XArray Dataset using `xr.open_dataset(fnm)`. \n",
    "Multiple files can be opened as a single XArray Dataset via `xr.open_mfdataset(fnm*.nc)`. \n",
    "`mf` stands for `multi-file`. Note \n",
    "the wildcard `fnm*` is supported. \n",
    "\n",
    "```\n",
    "def my_preprocessor(fds): return fds.swap_dims({'obs':'time'})\n",
    "\n",
    "ds = xr.open_mfdataset('files*.nc',                                \\\n",
    "                       preprocess = my_preprocessor,             \\\n",
    "                       concat_dim='time', combine='by_coords')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-compound",
   "metadata": {},
   "source": [
    "#### Obstacle: Getting information out of a Dataset\n",
    "\n",
    "There is a sort of comprehension / approach that I have found hard to internalize.\n",
    "With numpy ndarrays, XArray Datasets, etcetera there is this \"how do I get at it?\"\n",
    "problem. As this documentation evolves I will try and articulate the most helpful\n",
    "mindset. The starting point is that Datasets are built as collections of DataArrays; \n",
    "and these have an indexing protocol the merges with a method protocol (`sel`, `merge`\n",
    "and so on) where the end-result code that does what I want is inevitably very \n",
    "elegant. So it is a process of learning that elegant sub-language...\n",
    "\n",
    "\n",
    "#### Recover a time value as `datetime64` from a Dataset by index\n",
    "\n",
    "If `time` is a `dimension` it can be referenced via `ds.time[i]`. However\n",
    "this will be a 1-Dimensional, 1-element DataArray. Adding `.data`\n",
    "and casting the resulting ndarray (with one element) as a `dt64` works.\n",
    "\n",
    "```dt64(ds.time[i].data)```\n",
    "\n",
    "\n",
    "#### Example: XArray transformation flow\n",
    "    \n",
    "    \n",
    "As an example of the challenge of learning `XArray`: The reduction of this data to binned profiles\n",
    "requires a non-trivial workflow. A naive approach can result in a calculation that should take \n",
    "a seconds run for hours. (A key idea of this workflow -- the sortby() step -- is found on page 137 of **PDSH**.)\n",
    "    \n",
    "    \n",
    "- `swap_dims()` to substitute `pressure` for `time` as the ordinate dimension\n",
    "- `sortby()` to make the `pressure` dimension monotonic\n",
    "- Create a pressure-bin array to guide the subsequent data reduction\n",
    "- `groupby_bins()` together with `mean()` to reduce the data to a 0.25 meter quantized profile\n",
    "- use `transpose()` to re-order wavelength and pressure, making the resulting `DataArray` simpler to plot\n",
    "- accumulate these results by day as a list of `DataArrays`\n",
    "- From this list create an `XArray Dataset`\n",
    "- Write this to a new NetCDF file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Example: XArray Dataset subset and chart\n",
    "\n",
    "Time dimension slice:\n",
    "\n",
    "```\n",
    "ds = xr.open_dataset(\"file.nc\")\n",
    "ds = ds.sel(time=slice(t0, t1))\n",
    "ds\n",
    "```\n",
    "\n",
    "This shows that the temperature Data Variable has a cumbersome name: \n",
    "`sea_water_temperature_profiler_depth_enabled`. \n",
    "\n",
    "```\n",
    "ds = ds.rename({'sea_water_temperature_profiler_depth_enabled':'temperature'})\n",
    "```\n",
    "\n",
    "Plot this against the default dimension `time`:\n",
    "\n",
    "```\n",
    "ds.temperature.plot()\n",
    "```\n",
    "\n",
    "Temperature versus depth rather than time:\n",
    "\n",
    "```\n",
    "fig, axs = plt.subplots(figsize=(12,4), tight_layout=True)\n",
    "axs.plot(ds.temperature, -ds.z, marker='.', markersize=9., color='k', markerfacecolor='r')\n",
    "axs.set(ylim = (200., 0.), title='temperature against depth')\n",
    "```\n",
    "\n",
    "Here `ds.z` is negated to indicate depth below ocean surface.\n",
    "\n",
    "\n",
    "### More cleanup of Datasets: rename() and drop()\n",
    "\n",
    "* Use `ds = ds.rename(dictionary-of-from-to)` to rename data variables in a Dataset\n",
    "* Use `ds = ds.drop(string-name-of-data-var)` to get rid of a data variable\n",
    "* Use `ds = ds[[var1, var2]]` to eliminate all but those two variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-blackjack",
   "metadata": {},
   "source": [
    "### XArray ***DataArray*** name and length\n",
    "\n",
    "\n",
    "```\n",
    "sensor_t.name\n",
    "\n",
    "len(sensor_t)\n",
    "len(sensor_t.time)           # gives same result\n",
    "```\n",
    "\n",
    "What is the name of the controlling dimension?\n",
    "\n",
    "```\n",
    "if sensor_t.dims[0] == 'time': print('time is dimension zero')\n",
    "```\n",
    "\n",
    "Equivalent; but the second version permits reference by \"discoverable\" string.\n",
    "\n",
    "\n",
    "```\n",
    "sensor_t = ds_CTD_time_slice.seawater_temperature\n",
    "sensor_t = ds_CTD_time_slice['seawater_temperature']\n",
    "```\n",
    "\n",
    "#### Plotting with scaling and offsetting\n",
    "\n",
    "Suppose I wish to shift some data left to contrast it with some other data (where they would clobber one another)...\n",
    "\n",
    "```\n",
    "sensor_t + 0.4\n",
    "```\n",
    "\n",
    "Suppose I wish to scale some data in a chart to make it easier to interpret given a fixed axis range\n",
    "\n",
    "```\n",
    "sensor_t * 10.               # this fails by trying to make ten copies of the array\n",
    "\n",
    "np.ones(71)*3.*smooth_t      # this works by creating an inner product\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-series",
   "metadata": {},
   "source": [
    "## Pandas Series and DataFrames\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "### Summary\n",
    "\n",
    "### DataFrames\n",
    "\n",
    "DataFrames:\n",
    "\n",
    "* constructor takes `data=<ndarray>` and both `index` and `columns` arguments... \n",
    "    * ...2 dimensions only: higher dimensions and they say 'use XArray'\n",
    "    * ...and switching required a `.T` transpose\n",
    "* indexing by column and row header values, separated as in `[column_header][row_header]`\n",
    "    * as this reverses order from ndarrays: Better confirm... seems to be the case\n",
    "    * skip index/columns: defaults to integers.\n",
    " \n",
    "Below this section we go into n-dimensional arrays in Numpy, the *ndarray*. Here we take this \n",
    "for granted and look at the relationship with DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mediterranean-schedule",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ndarray from a list of lists (notice no comma delimiter):\n",
      "\n",
      " [['l' 'i' 's' 't' '1']\n",
      " ['s' 'c' 'n' 'd' '2']\n",
      " ['t' 'h' 'r' 'd' '3']] \n",
      "\n",
      "and indexing comparison: first [0][2] then [2][0]: s t\n",
      "\n",
      "and tuplesque indexing [0, 2] or [2, 0] equivalently gives: s t\n",
      "\n",
      "So ndarrays index [slow][fast] equivalent to [row][column]\n",
      "\n",
      "\n",
      "Moving on to DataFrames:\n",
      "\n",
      "\n",
      "     col_a col_b col_c col_d col_e\n",
      "2row     l     i     s     t     1\n",
      "4row     s     c     n     d     2\n",
      "6row     t     h     r     d     3 \n",
      "\n",
      "is a DataFrame from the ndarray; so now index [\"col_c\"][\"6row\"]: r\n",
      "\n",
      "Here is a Dataframe from a transpose of the ndarray\n",
      "\n",
      "       2row 4row 6row\n",
      "col_a    l    s    t\n",
      "col_b    i    c    h\n",
      "col_c    s    n    r\n",
      "col_d    t    d    d\n",
      "col_e    1    2    3 \n",
      "\n",
      "indexing 2row then col_e: 1\n",
      "\n",
      "So the column of a DataFrame is indexed first, then the row: Reverses the sense of the 2D ndarray.\n",
      "\n",
      "Now skipping the \"index=\"\" argument so the row labels default to integers:\n",
      "\n",
      "  col_a col_b col_c col_d col_e\n",
      "0     l     i     s     t     1\n",
      "1     s     c     n     d     2\n",
      "2     t     h     r     d     3 \n",
      "\n",
      "...so now indexing [\"col_d\"][0]: t \n",
      "\n",
      "      0  1  2  3  4\n",
      "2row  l  i  s  t  1\n",
      "4row  s  c  n  d  2\n",
      "6row  t  h  r  d  3 \n",
      "\n",
      "having done it the other way: used index= but not columns=. Here is element [0][\"4row\"]: s\n",
      "\n",
      "\n",
      "Starting from an XArray Dataset and using .to_dataframe() we arrive at a 2D structure.\n",
      "\n",
      "For example: df = ds_CTD.seawater_pressure.to_dataframe()\n",
      " \n",
      "The problem is that the resulting dataframe may not be indexed (row sense) using integers. A fix\n",
      "is necessary to override the index and columns attributes of the dataframe, as in:\n",
      " \n",
      "             df.index=range(len(df))\n",
      "             df.columns=range(1)\n",
      " \n",
      "results in a dataframe that one can index with integers [0] for column first then [n] for row.\n",
      "This example came from the profile time series analysis to get ascent start times and so on.\n",
      "The problem is it is a case of too much machinery. It is far simpler to use a pandas Series.\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "#\n",
    "# A micro study of ndarray to DataFrame translation\n",
    "#\n",
    "###################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Here is an ndarray construction from a built list of lists (not used in what follows): \n",
    "# arr = np.array([range(i, i+5) for i in [2, 4, 6]])                                       \n",
    "#     ... where the range() runs across columns; 2 4 6 are rows\n",
    "\n",
    "# ndarray construction: Notice all list elements are of the same type (strings)\n",
    "arr = np.array([['l','i','s','t','1'],['s','c','n','d','2'],['t','h','r','d', '3']])\n",
    "\n",
    "print('\\nndarray from a list of lists (notice no comma delimiter):\\n\\n', arr, \\\n",
    "      '\\n\\nand indexing comparison: first [0][2] then [2][0]:', arr[0][2], arr[2][0]) \n",
    "print('\\nand tuplesque indexing [0, 2] or [2, 0] equivalently gives:', arr[0,2], arr[2,0])\n",
    "print('\\nSo ndarrays index [slow][fast] equivalent to [row][column]\\n\\n\\nMoving on to DataFrames:\\n\\n')\n",
    "\n",
    "rowlist=[\"2row\", \"4row\", \"6row\"]\n",
    "columnlist = [\"col_a\", \"col_b\", \"col_c\", \"col_d\", \"col_e\"]\n",
    "df = pd.DataFrame(data=arr, index=rowlist, columns=columnlist)\n",
    "\n",
    "print(df, '\\n\\nis a DataFrame from the ndarray; so now index [\"col_c\"][\"6row\"]:', df['col_c']['6row'])\n",
    "\n",
    "df = pd.DataFrame(data=arr.T, index=columnlist, columns=rowlist)\n",
    "\n",
    "print('\\nHere is a Dataframe from a transpose of the ndarray\\n\\n', df, \\\n",
    "      '\\n\\nindexing 2row then col_e:', df['2row']['col_e'])\n",
    "print('\\nSo the column of a DataFrame is indexed first, then the row: Reverses the sense of the 2D ndarray.\\n')\n",
    "print('Now skipping the \"index=\"\" argument so the row labels default to integers:\\n')\n",
    "\n",
    "df = pd.DataFrame(data=arr, columns=columnlist)\n",
    "\n",
    "print(df, '\\n\\n...so now indexing [\"col_d\"][0]:', df['col_d'][0], '\\n')\n",
    "\n",
    "df = pd.DataFrame(data=arr, index=rowlist)\n",
    "\n",
    "print(df, '\\n\\nhaving done it the other way: used index= but not columns=. Here is element [0][\"4row\"]:', \\\n",
    "      df[0]['4row'])\n",
    "\n",
    "\n",
    "print('\\n\\nStarting from an XArray Dataset and using .to_dataframe() we arrive at a 2D structure.\\n')\n",
    "print('For example: df = ds_CTD.seawater_pressure.to_dataframe()')\n",
    "print(' ')\n",
    "print('The problem is that the resulting dataframe may not be indexed (row sense) using integers. A fix')\n",
    "print('is necessary to override the index and columns attributes of the dataframe, as in:')\n",
    "print(' ')\n",
    "print('             df.index=range(len(df))')\n",
    "print('             df.columns=range(1)')\n",
    "print(' ')\n",
    "print('results in a dataframe that one can index with integers [0] for column first then [n] for row.')\n",
    "print('This example came from the profile time series analysis to get ascent start times and so on.')\n",
    "print('The problem is it is a case of too much machinery. It is far simpler to use a pandas Series.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d6a6de",
   "metadata": {},
   "source": [
    "### Selecting based on a range\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "Suppose we have a DataFrame with a column of timestamps over a broad time range and we would like to focus on only a subset. \n",
    "One approach would be to generate a smaller dataframe that meets the small time criterion and iterate over that.\n",
    "\n",
    "The following cell builds a pandas DataFrame with a date column; then creates a subset DataFrame where only rows in\n",
    "a time range are preserved. This is done twice: First using conditional logic and then using the same with '.loc'. \n",
    "('loc' and 'iloc' are location-based indexing, the first relying on labels and the second on integer position.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a58cc95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[numpy.datetime64('2020-10-11') 7 13 6]\n",
      " [numpy.datetime64('2020-10-12') 7 9 6]\n",
      " [numpy.datetime64('2020-10-13') 7 8 6]\n",
      " [numpy.datetime64('2020-10-14') 7 5 6]\n",
      " [numpy.datetime64('2020-10-15') 7 11 6]]\n",
      "\n",
      "arr[0][2] then [2][0]: 13 2020-10-13\n",
      "\n",
      "and tuplesque indexing [0, 2] or [2, 0] equivalently gives: 13 2020-10-13\n",
      "\n",
      "using conditionals:\n",
      "\n",
      "            date data1 data2 data3\n",
      "day3 2020-10-13     7     8     6\n",
      "day4 2020-10-14     7     5     6 \n",
      "\n",
      "\n",
      "using loc:\n",
      "\n",
      "            date data1 data2 data3\n",
      "day3 2020-10-13     7     8     6\n",
      "day4 2020-10-14     7     5     6\n",
      "\n",
      "notice the results are identical; so it is an open question \"Why use `loc`?\"\n"
     ]
    }
   ],
   "source": [
    "from numpy import datetime64 as dt64, timedelta64 as td64\n",
    "\n",
    "t0=dt64('2020-10-11')\n",
    "t1=dt64('2020-10-12')\n",
    "t2=dt64('2020-10-13')\n",
    "t3=dt64('2020-10-14')\n",
    "t4=dt64('2020-10-15')\n",
    "\n",
    "r0 = dt64('2020-10-12')\n",
    "r1 = dt64('2020-10-15')\n",
    "\n",
    "arr = np.array([[t0,7,13,6],[t1,7,9,6],[t2,7,8,6],[t3,7,5,6],[t4,7,11,6]])\n",
    "\n",
    "print(arr)\n",
    "print('\\narr[0][2] then [2][0]:', arr[0][2], arr[2][0]) \n",
    "print('\\nand tuplesque indexing [0, 2] or [2, 0] equivalently gives:', arr[0,2], arr[2,0])\n",
    "\n",
    "rowlist    = [\"day1\", \"day2\",\"day3\",\"day4\",\"day5\"]\n",
    "columnlist = [\"date\", \"data1\", \"data2\", \"data3\"]\n",
    "df = pd.DataFrame(data=arr, index=rowlist, columns=columnlist)\n",
    "\n",
    "\n",
    "df_conditional = df[(df['date'] > r0) & (df['date'] < r1)]\n",
    "print('\\nusing conditionals:\\n\\n', df_conditional, '\\n')\n",
    "\n",
    "\n",
    "df_loc = df.loc[(df['date'] > r0) & (df['date'] < r1)]\n",
    "print('\\nusing loc:\\n\\n', df_loc)\n",
    "\n",
    "print('\\nnotice the results are identical; so it is an open question \"Why use `loc`?\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f3689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "celtic-following",
   "metadata": {},
   "source": [
    "## Numpy ndarrays\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "### Summary\n",
    "\n",
    "### Diving in \n",
    "\n",
    "numpy ndarrays \n",
    "\n",
    "* do not have row and column headers; whereas pandas DataFrames do have typed headers\n",
    "* indexing has an equivalence of `[2][0]` to `[2,0]` \n",
    "    * The latter (with comma) is the presented way in PDSH\n",
    "    * This duality does not work for DataFrames\n",
    "* has row-then-column index order...\n",
    "    * ....with three rows in `[['l','i','s','t','1'],['s','c','n','d','2'],['t','h','r','d','3']]` \n",
    "* has slice by dimension as `start:stop:step` by default `0, len (this dimension), 1` \n",
    "    * ...exception: when `step` is negative `start` and `stop` are reversed\n",
    "    * ...multi-dimensional slices separated by commas\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-trade",
   "metadata": {},
   "source": [
    "## Time\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "There is time in association with data (when a sample was recorded) and time in association with\n",
    "code development (how long did this cell take to run?) Let's look at both.\n",
    "\n",
    "\n",
    "### Sample timing\n",
    "\n",
    "See PDSH-189. There are two time mechanisms in play: Python's built-in `datetime` and an improvement called\n",
    "`datetime64` from **numpy** that enables *arrays* of dates, i.e. time series. \n",
    "\n",
    "\n",
    "Consider these two ways of stipulating time slice arguments for `.sel()` applied to a DataSet.\n",
    "First:  Use a datetime64 with precision to minutes (or finer).\n",
    "Second: Pass strings that are interpreted as days, inclusive. In pseudo-code: \n",
    "\n",
    "```\n",
    "if do_precision:  \n",
    "   t0 = dt64('2019-06-01T00:00')\n",
    "   t1 = dt64('2019-06-01T05:20')\n",
    "   dss = ds.sel(time=slice(t0, t1))   \n",
    "else:\n",
    "    day1 = '24'\n",
    "    day2 = '27'              # will be 'day 27 inclusive' giving four days of results\n",
    "    dss = ds.sel(time=slice('2019-06-' + day1, '2019-08-' + day2))\n",
    "\n",
    "len(dss.time)\n",
    "```\n",
    "\n",
    "### Execution timing\n",
    "\n",
    "Time of execution in seconds: \n",
    "\n",
    "```\n",
    "from time import time\n",
    "\n",
    "toc = time()\n",
    "for i in range(12): j = i + 1\n",
    "tic = time()\n",
    "print(tic - toc)\n",
    "\n",
    "7.82012939453125e-05\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-sellers",
   "metadata": {},
   "source": [
    "## ipywidgets\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "\n",
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-toolbox",
   "metadata": {},
   "source": [
    "## Holoview\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "\n",
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-vintage",
   "metadata": {},
   "source": [
    "## Process these notes\n",
    "\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "\n",
    "\n",
    "### Open issues\n",
    "\n",
    "* Does re-sample to (say) minutes over a time where data are missing create skips? Or nans? \n",
    "* What does merge() do? \n",
    "\n",
    "```\n",
    "ds = ds.merge(ds_flort)\n",
    "```\n",
    "\n",
    "* Order: `.merge()` then `.resample()` with `mean()`; or vice versa? (existing code is vice-versa)\n",
    "    * This approach does resampling prior to merge but was taking way too long\n",
    "\n",
    "```\n",
    "ds = ds_flort.copy()\n",
    "ds = ds.reset_coords('seawater_pressure')        # converts the coordinate to a data variable\n",
    "ds_mean = ds.resample(time='1Min').mean()\n",
    "ds_std  = ds.resample(time='1Min').std()\n",
    "```\n",
    "\n",
    "* How to copy a dataset, move a coordinate to a data variable\n",
    "\n",
    "```\n",
    "ds = ds_ctdpf.copy()\n",
    "ds = ds.reset_coords('seawater_pressure')        # converts the coordinate to a data variable\n",
    "```\n",
    "\n",
    "* Standard deviation method\n",
    "\n",
    "```\n",
    "ds_std  = ds.resample(time='1Min').std()\n",
    "```\n",
    "\n",
    "* Load R/M Dataset ctdpf + flort\n",
    "    * Some data are noisier towards the surface, some are ridiculously noisy. \n",
    "    * Idea: Filter on standard deviation, threshold:\n",
    "        * Filter leaves a signal of interest? \n",
    "    * Fluorometer is particularly troublesome. \n",
    "\n",
    "\n",
    "* Depth profiles reduce to simple metrics\n",
    "    * profile start, peak, end times\n",
    "    * platform residence: start and end times (from profile times)\n",
    "    * (smoothed) chlorophyll derivative, curvature, rate of curvature\n",
    "    * Similarly salinity seems to go through a consistent double-zero in rate of curvature\n",
    "    * intersection depth as used in TDR; for example for temperature or salinity\n",
    "        * extrapolate smoothed pressure by backing off the derivative change\n",
    "        * extrapolate platform, intersect\n",
    "    * time of day / sun angle\n",
    "    * local time\n",
    "    * rate of ascent verify; 3m / minute?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-wesley",
   "metadata": {},
   "source": [
    "## Spectrophotometer (SP) and Nitrate\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "The SP runs on ascent only, at about 3.7 samples per second. Compare nitrate that also runs \n",
    "on ascent only at about 3 samples per minute. Nitrate data is fairly straightforward; SP \n",
    "data is chaotic/messy. The objective is to reduce the SP to something interpretable.\n",
    "\n",
    "\n",
    "### Deconstructing data: process pattern\n",
    "\n",
    "\n",
    "- `ds = xr.open_dataset(fnm)` \n",
    "    - Data dispersed across files: Variant + wildcard: `ds = xr.open_mfdataset('data_with_*_.nc')`\n",
    "- `obs` dimensional coordinate creates degeneracy over multiple files\n",
    "    - Use `.swap_dims` to swap time for `obs`\n",
    "- `ds.time[0].values, ds.time[-1].values` gives a timespan but nothing about duty cycles\n",
    "    - 2019 spectrophotometer data at Oregon Slope Base: 86 channels, 7 million samples\n",
    "    - ...leading to...\n",
    "        - Only operates during midnight and noon ascent; at 3.7 samples per second\n",
    "        - Optical absorbance and beam attenuation are the two data types\n",
    "        - Data has frequent dropouts over calendar time\n",
    "        - Data has spikes that tend to register across all 86 channels\n",
    "        - Very poor documentation; even the SME report is cursory\n",
    "\n",
    "\n",
    "    \n",
    "### Nitrate \n",
    "\n",
    "    \n",
    "This code follows suit the spectrophotometer. It is simpler because there is only a nitrate value \n",
    "and no wavelength channel. \n",
    "\n",
    "    \n",
    "I kept the pressure bins the same even though the nitrate averates about 3 three samples or less per minute\n",
    "during a 70 minute ascent. That's about three meters per minute so one sample per meter. Since the \n",
    "spectrophotometer bin depth is 0.25 meters there are necessarily a lot of empty bins (bins with no data)\n",
    "for the nitrate profile. \n",
    "\n",
    "    \n",
    "### Two open issues\n",
    "\n",
    "\n",
    "A curious artifact of the situation is from a past bias: I had understood that the SCIP makes pauses \n",
    "on descent to accommodate the nitrate sensor. I may be in error but now it looks like this sensor, \n",
    "the nitrate sensor, is observing on ascent which is continuous. This leaves open the question of \n",
    "why the pauses occur on the descent. If I have that right. \n",
    "\n",
    "\n",
    "Finally there are two nitrate signals: 'samp' and 'dark'. This code addresses only 'samp' as 'dark'\n",
    "is showing nothing of interest. So this is an open issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-invasion",
   "metadata": {},
   "source": [
    "##### to do \n",
    "\n",
    "- Add pco2\n",
    "- Write a data characterization report with simple questions answered\n",
    "    - The example I have in mind is 'does this sensor respond to ambient light?' and 'should it?'\n",
    "- Treat profiles as monolithic in time (philosophical); see dimension swapping code example below in this cell\n",
    "- set up running averages\n",
    "- smear / chooser\n",
    "- extend the use of time delimeters across everything that follows\n",
    "- get rid of the print...s and the other junk print\n",
    "- look at the nitrate doy usage and maybe shift to that\n",
    "- deal with \"at rest\": Is data still accumulated?\n",
    "- deal with platform coincident data: agrees with profiler?\n",
    "- Review the process of setting up all these refined source datasets...\n",
    "    - What if I don't *want* to look at January 2019\n",
    "    - What is the operational record of all the shallow profilers? \n",
    "- What is the shape of January? (for each sensor: What is the data \"box\"?)\n",
    "- Starting with pH let's consider colorized curtain plots as an important goal\n",
    "- Plot irradiance (7 options) versus PAR\n",
    "- Deal with two older images:\n",
    "    - ./images/misc/optaa_spectra_0_10_20_JAN_2019.png\n",
    "    - ./images/misc/nitrate_2019_JAN_1_to_10.png\n",
    "- Full review / integration of instrument notes into corresponding vis code blocks; see below this cell\n",
    "\n",
    "\n",
    "***Important: pH sensor fire once at the end of every profile; back in the platform***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Instrumentation notes: Needs integration (see to do above)\n",
    "\n",
    "\n",
    "Manufacturer etc: [here](https://interactiveoceans.washington.edu/instruments/).\n",
    "\n",
    "\n",
    "A set of files loads as a single xarray *Dataset* comprised of multiple *DataArrays*.\n",
    "Writing out the Dataset gives DataArray names; but the DataArray can itself be invoked \n",
    "with `.attrs` to see additional attributes that are invisible when looking at Dataset \n",
    "attributes. This is useful for designing data simplification. \n",
    "\n",
    "```\n",
    "ds\n",
    "````\n",
    "\n",
    "...lists the DataArrays in the Dataset. \n",
    "\n",
    "```\n",
    "ds.density.attrs\n",
    "```\n",
    "\n",
    "...lists the attributes of the `density` DataArray.\n",
    "\n",
    "\n",
    "\n",
    "##### Optical absorption spectrophotometer\n",
    "\n",
    "\n",
    "* Seabird Scientific from acquisition of WETLABS: ac-s model\n",
    "* Devices mounted on the shallow profilers\n",
    "* 86 wavelengths per sample; in practice some nan values at both ends\n",
    "* Operates only during shallow profiler ascents\n",
    "  * Only on the two \"nitrate\" ascents each day\n",
    "  * Data sample is about one per 0.27 seconds\n",
    "  * However it often does a \"skip\" with a sample interval about 0.5 seconds\n",
    "  * The nitrate run ascent is ~62 minutes (ascent only); ~3 meters per minute\n",
    "  * Ascent is about 14,000 samples; so 220 samples per minute\n",
    "  * That is 70 samples per meter depth over 20 seconds\n",
    "* Per the User's Manual post-processing gets rather involved\n",
    "* Spectral absorption: parameter `a`, values typically 20 - 45. \n",
    "* Attenuation is `c` with values on 0 to 1.\n",
    "* Coordinates we want are `time`, `int_ctd_pressure`, `wavelength`\n",
    "  * `time` and `wavelength` are also dimensions\n",
    "* Data variables we want are `beam_attenuation` (this is `c`) and `optical_absorption` (`a`)\n",
    "* Per year data is about 1.7 billion floating point numbers\n",
    "  * 86 wavelengths x 2 (c, a) x 2 (ascent / day) x 14,000 (sample / ascent) x 365\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Photosynthetically Active Radiation (PAR)\n",
    "\n",
    "\n",
    "* Devices mounted on the shallow profiler and the SP platform\n",
    "* Seabird Scientific (from acquisition of Satlantic): PAR model\n",
    "* Some ambiguity in desired result: `par`, `par_measured` and `par_counts_output` are all present in the data file\n",
    "  * Since `qc` values are associated with it I will simply use `par_counts_output`\n",
    "\n",
    "\n",
    "##### Fluorometer\n",
    "\n",
    "\n",
    "* WETLABS (Seabird Scientific from acquisition) Triplet\n",
    "* Chlorophyll emission is at 683 nm\n",
    "* Measurement wavelengths in nm are 700.0 (scattering), 460.0 (cdom) and 695.0 (chlorophyll)\n",
    "* Candidate Data variables\n",
    "  * Definites are `fluorometric_chlorophyll_a` and `fluorometric_cdom`\n",
    "  * Possibles are `total_volume_scattering_coefficient`, `seawater_scattering_coefficient`, `optical_backscatter`\n",
    "    * qc points to total volume scattering and optical backscatter but I'll keep all three\n",
    "\n",
    "\n",
    "##### Nitrate (nutnr_a_sample and nutnr_a_dark_sample)\n",
    "\n",
    "\n",
    "##### pCO2 water (two streams: pco2w_b_sami_data_record and pco2w_a_sami_data_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-niagara",
   "metadata": {},
   "source": [
    "```\n",
    "####################\n",
    "#\n",
    "# Nitrate\n",
    "# \n",
    "#   dims:       time\n",
    "#   coords:     time and int_ctd_pressure\n",
    "#   data array: nitrate concentration\n",
    "#\n",
    "# To do\n",
    "#   identify when the data happens\n",
    "#   verify that the 'dark' means nothing...\n",
    "# \n",
    "####################\n",
    "\n",
    "ds_n03dark = xr.open_dataset(\"/data/rca/simpler/osb_sp_nutnr_a_dark_2019.nc\")\n",
    "ds_n03samp = xr.open_dataset(\"/data/rca/simpler/osb_sp_nutnr_a_sample_2019.nc\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "include_charts = False\n",
    "\n",
    "m_strs = ['01', '02', '03', '04', '05', '06', '07', '08', '09']           # relevant 2019 months\n",
    "m_days = [31, 28, 31, 30, 31, 30, 31, 31, 30]                             # days per month in 2019\n",
    "\n",
    "month_index = 0                                                           # manage time via months and days; 0 is January\n",
    "month_str   = m_strs[month_index]  \n",
    "year_str    = '2019'\n",
    "\n",
    "n_meters          = 200\n",
    "n_bins_per_meter  = 4\n",
    "halfbin           = (1/2) * (1/n_bins_per_meter)\n",
    "n_pressure_bins   = n_meters * n_bins_per_meter\n",
    "p_bounds          = np.linspace(0., n_meters, n_pressure_bins + 1)             # 801 bounds: 0., .25, ..., 200.                   \n",
    "pressure          = np.linspace(halfbin, n_meters - halfbin, n_pressure_bins)  # 800 centers: 0.125, ..., 199.875                  \n",
    "nc_upper_bound    = 40.\n",
    "\n",
    "ndays = m_days[month_index]\n",
    "ndayplots, dayplotdays = 10, list(range(10))\n",
    "\n",
    "l_da_nc_midn, l_da_nc_noon = [], []       # these lists accumulate DataArrays by day\n",
    "\n",
    "if include_charts:\n",
    "    fig_height, fig_width, fig_n_across, fig_n_down = 4, 4, 2, ndayplots\n",
    "    fig, axs = plt.subplots(ndayplots, fig_n_across, figsize=(fig_width * fig_n_across, fig_height * fig_n_down), tight_layout=True)\n",
    "\n",
    "for day_index in range(ndays):\n",
    "    \n",
    "    day_str  = day_of_month_to_string(day_index + 1); date_str = year_str + '-' + month_str + '-' + day_str\n",
    "    this_doy = doy(dt64(date_str))\n",
    "    clear_output(wait = True); print(\"on day\", day_str, 'i.e. doy', this_doy)\n",
    "    midn_start = date_str + 'T07:00:00'\n",
    "    midn_done  = date_str + 'T10:00:00'\n",
    "    noon_start = date_str + 'T20:00:00'\n",
    "    noon_done  = date_str + 'T23:00:00'\n",
    "\n",
    "    # pull out OA and BA for both midnight and noon ascents; and swap in pressure for time\n",
    "    ds_midn = ds_n03samp.sel(time=slice(dt64(midn_start), dt64(midn_done))).swap_dims({'time':'int_ctd_pressure'})\n",
    "    ds_noon = ds_n03samp.sel(time=slice(dt64(noon_start), dt64(noon_done))).swap_dims({'time':'int_ctd_pressure'})\n",
    "    \n",
    "    # print('pressures:', ds_midn.int_ctd_pressure.size, ds_noon.int_ctd_pressure.size, '; times:', ds_midn.time.size, ds_noon.time.size)    \n",
    "    midn = True if ds_midn.time.size > 0 else False\n",
    "    noon = True if ds_noon.time.size > 0 else False\n",
    "        \n",
    "    if midn:\n",
    "        da_nc_midn = ds_midn.nitrate_concentration.expand_dims({'doy':[this_doy]})\n",
    "        del da_nc_midn['time']\n",
    "        l_da_nc_midn.append(da_nc_midn.sortby('int_ctd_pressure').groupby_bins(\"int_ctd_pressure\", p_bounds, labels=pressure).mean().transpose('int_ctd_pressure_bins', 'doy'))\n",
    "        \n",
    "    if noon:\n",
    "        da_nc_noon = ds_noon.nitrate_concentration.expand_dims({'doy':[this_doy]})\n",
    "        del da_nc_noon['time']\n",
    "        l_da_nc_noon.append(da_nc_noon.sortby('int_ctd_pressure').groupby_bins(\"int_ctd_pressure\", p_bounds, labels=pressure).mean().transpose('int_ctd_pressure_bins', 'doy'))\n",
    "\n",
    "    if include_charts and day_index in dayplotdays:      # if this is a plotting day: Add to the chart repertoire\n",
    "\n",
    "        dayplotindex = dayplotdays.index(day_index) \n",
    "\n",
    "        if midn:\n",
    "            axs[dayplotindex][0].scatter(l_da_nc_midn[-1], pressure,  marker=',', s=2., color='r') \n",
    "            axs[dayplotindex][0].set(xlim = (.0, nc_upper_bound), ylim = (200., 0.), title='NC midnight')\n",
    "            axs[dayplotindex][0].scatter(ds_midn.nitrate_concentration, ds_midn.int_ctd_pressure, marker=',', s=1., color='b'); \n",
    "            \n",
    "        if noon:\n",
    "            axs[dayplotindex][1].scatter(l_da_nc_noon[-1], pressure,  marker=',', s=2., color='g')\n",
    "            axs[dayplotindex][1].set(xlim = (.0, nc_upper_bound), ylim = (200., 0.), title='NC noon')\n",
    "            axs[dayplotindex][1].scatter(ds_noon.nitrate_concentration, ds_noon.int_ctd_pressure, marker=',', s=1., color='k'); \n",
    "\n",
    "save_figure = False\n",
    "if save_figure: fig.savefig('/home/ubuntu/chlorophyll/images/misc/nitrate_2019_JAN_1_to_10.png')\n",
    "\n",
    "save_nitrate_profiles = False\n",
    "\n",
    "if save_nitrate_profiles: \n",
    "    ds_nc_midn = xr.concat(l_da_nc_midn, dim=\"doy\").to_dataset(name='nitrate_concentration')\n",
    "    ds_nc_noon = xr.concat(l_da_nc_noon, dim=\"doy\").to_dataset(name='nitrate_concentration')\n",
    "\n",
    "    ds_nc_midn.to_netcdf(\"/data1/nutnr/nc_midn_2019_01.nc\")\n",
    "    ds_nc_noon.to_netcdf(\"/data1/nutnr/nc_noon_2019_01.nc\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "super-pocket",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML, Video   \u001b[38;5;66;03m# HTML is ...?...\u001b[39;00m\n\u001b[1;32m     30\u001b[0m                                           \u001b[38;5;66;03m# Video is used for load/playback\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# First set up the figure, the axis, and the plot element we want to animate\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots()\n\u001b[1;32m     35\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_xlim(( \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     36\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Animation in Python is one thing. Animation in a Jupyter notebook is another.\n",
    "# Animation in binder is yet another. Rather than try and bootstrap a lesson here\n",
    "# I present a sequence of annotated steps that create an animation, save it as \n",
    "# an .mp4 file, load it and run it: In a Jupyter notebook of course. Then we\n",
    "# will see how it does in binder.\n",
    "\n",
    "# At some point in working on this I did a conda install ffmpeg. I am not clear \n",
    "#   right now on whether this was necessary or not; I suspect not.\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# With [the inline] backend activated with this line magic matplotlib command, the output \n",
    "# of plotting commands is displayed inline within frontends like the Jupyter notebook, \n",
    "# directly below the code cell that produced it. The resulting plots will then also be stored \n",
    "# in the notebook document.\n",
    "\n",
    "# de rigeur, commented out here as this runs at the top of the notebook\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import animation, rc      # animation provides tools to build chart-based animations.\n",
    "                                          # Each time Matplotlib loads, it defines a runtime configuration (rc) \n",
    "                                          #   containing the default styles for every plot element you create. \n",
    "                                          #   This configuration can be adjusted at any time using \n",
    "                                          #   the plt. ... matplotlibrc file, which you can read about \n",
    "                                          #   in the Matplotlib documentation.\n",
    "\n",
    "\n",
    "from IPython.display import HTML, Video   # HTML is ...?...\n",
    "                                          # Video is used for load/playback\n",
    "\n",
    "# First set up the figure, the axis, and the plot element we want to animate\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlim(( 0, 2))\n",
    "ax.set_ylim((-2, 2))\n",
    "\n",
    "line, = ax.plot([], [], lw=2)\n",
    "\n",
    "# initialization function: plot the background of each frame\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    return (line,)\n",
    "\n",
    "# animation function. This is called sequentially\n",
    "def animate(i):\n",
    "    x = np.linspace(0, 2, 1000)\n",
    "    y = np.sin(2 * np.pi * (x - 0.01 * i))\n",
    "    line.set_data(x, y)\n",
    "    return (line,)\n",
    "\n",
    "# call the animator. blit=True means only re-draw the parts that have changed.\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=100, interval=12, blit=True)\n",
    "\n",
    "HTML(anim.to_html5_video())\n",
    "\n",
    "# print(anim._repr_html_() is None) will be True\n",
    "# anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_line(num, data, line):\n",
    "    line.set_data(data[..., :num])\n",
    "    return line,\n",
    "\n",
    "fig1 = plt.figure()\n",
    "\n",
    "data = .05 + 0.9*np.random.rand(2, 200)\n",
    "l, = plt.plot([], [], 'r-')                # l, takes the 1-tuple returned by plt.plot() and grabs that first \n",
    "                                           # and only element; so it de-tuples it\n",
    "\n",
    "plt.xlim(0, 1); plt.ylim(0, 1); plt.xlabel('x'); plt.title('test')\n",
    "\n",
    "lines_anim = animation.FuncAnimation(fig1, update_line, 200, fargs=(data, l), interval=1, blit=True)\n",
    "\n",
    "# fargs are additional arguments to 'update_line()' in addition to the frame number: data and line\n",
    "# interval is a time gap between frames (guess is milliseconds)\n",
    "# blit is the idea of modifying only pixels that change from one frame to the next\n",
    "\n",
    "# For direct display use this: HTML(line_ani.to_html5_video())\n",
    "lines_anim.save('./lines_tmp3.mp4')            # save the animation to a file\n",
    "Video(\"./lines_tmp3.mp4\")                      # One can add , embed=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig2 = plt.figure()\n",
    "\n",
    "x = np.arange(-9, 10)\n",
    "y = np.arange(-9, 10).reshape(-1, 1)\n",
    "base = np.hypot(x, y)\n",
    "ims = []\n",
    "for add in np.arange(15):\n",
    "    ims.append((plt.pcolor(x, y, base + add, norm=plt.Normalize(0, 30)),))\n",
    "\n",
    "im_ani = animation.ArtistAnimation(fig2, ims, interval=50, repeat_delay=3000,\n",
    "                                   blit=True)\n",
    "# To save this second animation with some metadata, use the following command:\n",
    "# im_ani.save('im.mp4', metadata={'artist':'Guido'})\n",
    "\n",
    "HTML(im_ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-client",
   "metadata": {},
   "source": [
    "## Binder\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "* Create a binder badge in the home page `README.md` of the repository. \n",
    "\n",
    "```\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/<accountname>/<reponame>/HEAD)\n",
    "\n",
    "```\n",
    "\n",
    "* In `<repo>/binder` create `environment.yml` to match the working environment\n",
    "    * For this repo as of 10/23/2021 `binder/environment.yml` was: \n",
    "\n",
    "\n",
    "```\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - matplotlib\n",
    "  - netcdf4\n",
    "  - xarray\n",
    "  - ffmpeg\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-money",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53e4aa79-2023-47b6-98ee-3d9890f76808",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "This notebook tries to describe the technical tasks in the context of 'why are they necessary'. \n",
    "\n",
    "\n",
    "## Technical topics covered\n",
    "\n",
    "- Working with shallow profiler ascent/descent/rest cycles\n",
    "- bash, text editor, git, GitHub\n",
    "- running a Jupyter notebook server (code and markdown)\n",
    "- Ordering, retrieving and cleaning datasets from OOI\n",
    "- Deconstructing datasets into usable chunks\n",
    "- Basic Python as the baseline layer of the code\n",
    "    - Start by putting all code in notebooks\n",
    "    - Notebooks can become over-long or very code-dense so...\n",
    "        - Be prepared to break notebooks apart into smaller notebooks\n",
    "        - Be prepared to migrate blocks of working code to module files\n",
    "- Install Python data science libraries for plotting, opening datasets, slicing\n",
    "    - starting with `matplotlib`, `numpy`, `pandas`, and `xarray`\n",
    "- As needed bring in Python extension libraries\n",
    "    - interactive widgets, maps, animation, color maps\n",
    "- Pulling other data (besides shallow profiler) from the OOI data system\n",
    "- Pulling datasets from other programs: ARGO, MODIS, GLODAP, ROMS, MSLA, etcetera\n",
    "- Using binder as an ephemeral executable sandbox\n",
    "- Working from larger extra-repo datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec3b328-70bf-471f-864d-66724b9486d4",
   "metadata": {},
   "source": [
    "### Working with shallow profiler ascent/descent/rest cycles\n",
    "\n",
    "\n",
    "This topic is out of sequence intentionally. The topics that follow are in more of a logical order.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The issue at hand is that the shallow profiler ascends and descends and rests about nine times per\n",
    "day; but the time of day when these events happen is not perfectly fixed. As a result we need \n",
    "a means to identify the start and end times of (say) an ascent so that we can be confident that\n",
    "the data were in fact acquired as the profiler ascended through the water column. This is also \n",
    "useful for comparing ascent to descent data or comparing profiler-at-rest data to platform data\n",
    "(since the profiler is at rest *on* the platform).\n",
    "\n",
    "\n",
    "\n",
    "To restate the task: From a conceptual { time window } we would like very specific { metadata }\n",
    "for time windows when the profiler ascended while collecting data. \n",
    "That is, we want accurate subsidiary time windows for successive profiles within our conceptual\n",
    "time window; per site and year.\n",
    "We can then use these specific { time window } boundaries to select data\n",
    "subsets from corresponding profiling runs. \n",
    "\n",
    "\n",
    "\n",
    "The first step in this process is to get CTD data for the shallow profiler since it will have a\n",
    "record of depth over time. This record is scanned in one-year chunks to identify the UTM start\n",
    "times of each successive profile. Also determined: The start times of descents and the start times of rests. \n",
    "\n",
    "\n",
    "\n",
    "From these three sets of timestamps we can make the assumption that the end of an \n",
    "ascent corresponds to the start of a descent. Likewise the end of a descent is the start of \n",
    "a rest; and the start of an ascent is the end of the previous rest. Each ascent / descent / rest\n",
    "interval is considered as one profile (in that order). The results are written to a CSV file\n",
    "that has one row of timing metadata per profile. \n",
    "\n",
    "\n",
    "\n",
    "Now suppose the goal is to create a sequence of temperature plots for July 2019 for the Axial \n",
    "Base shallow profiler. First we would identify the pre-existing CSV file for Axial Base for the\n",
    "year 2019 and read that file into a pandas Dataframe. Let's suppose it is read into a profile\n",
    "Dataframe called `p` and that we have labled the six columns that correspond to\n",
    "ascent start/end, descent start/end and rest start/ned. Here is example code from `BioOpticsModule.py`.\n",
    "\n",
    "\n",
    "```\n",
    "p = pd.read_csv(metadata_filename, usecols=[\"1\", \"3\", \"5\", \"7\", \"9\", \"11\"])\n",
    "p.columns          = ['ascent_start', 'ascent_end', 'descent_start', 'descent_end', 'rest_start', 'rest_end']\n",
    "p['ascent_start']  = pd.to_datetime(pDf['ascent_start'])\n",
    "p['ascent_end']    = pd.to_datetime(pDf['ascent_end'])\n",
    "p['descent_start'] = pd.to_datetime(pDf['descent_start'])\n",
    "p['descent_end']   = pd.to_datetime(pDf['descent_end'])\n",
    "p['rest_start']    = pd.to_datetime(pDf['rest_start'])\n",
    "p['rest_end']      = pd.to_datetime(pDf['rest_end'])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Let's examine two rows of this Dataframe:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "print(p['ascent_start'][0])\n",
    "\n",
    "2019-01-01 00:27:00\n",
    "\n",
    "print(p['ascent_start'][1600])\n",
    "\n",
    "2019-07-04 15:47:00\n",
    "```\n",
    "\n",
    "\n",
    "That is, row 0 corresponds to the start of 2019, January 1, and row 1600 occurs on July 4.\n",
    "\n",
    "\n",
    "For a 365 day year with no\n",
    "missed profiles (9 profiles per day) this file would contain 365 * 9 = 3285 profiles. In practice\n",
    "there will be fewer owing to storms or other factors that interrupt data acquisition. \n",
    "\n",
    "\n",
    "Each row of this dataframe corresponds to a profile run (ascent, descent, rest) of the shallow\n",
    "profiler. Consequently we could use the time boundaries of one such row to select data that was\n",
    "acquired *during the ascent period of that profile*. Suppose a temperature dataset for the month of July \n",
    "is called `T`. `T` is constructed as an xarray Dataset with dimension `time`. \n",
    "We can use the xarray *select* method `.sel`, as in `T.sel(time=slice(time0, time1))`, to\n",
    "produce a Dataset with only times \n",
    "that fall within a profile ascent window.  \n",
    "\n",
    "\n",
    "```\n",
    "time0    = p['ascent_start'][1600]\n",
    "time1    = p['ascent_end'][1600]\n",
    "T_ascent = T.sel(time=slice(time0, time1))\n",
    "```\n",
    "\n",
    "\n",
    "Now `T_ascent` will contain about 60 minutes worth of data. \n",
    "\n",
    "\n",
    "\n",
    "This demonstrates loading time boundaries from the metadata `p`. \n",
    "The metadata informs the small time box. Now we need the other direction \n",
    "as well: Suppose the interval of interest is the first four days of July 2019.\n",
    "We have no idea which rows of the metadata `p` this corresponds to. We need\n",
    "a list of row indices for `p` in that time window. For this we \n",
    "have a utility function.\n",
    "\n",
    "\n",
    "```\n",
    "def GenerateTimeWindowIndices(pDf, date0, date1, time0, time1):\n",
    "    '''\n",
    "    Given two day boundaries and a time window (UTC) within a day: Return a list\n",
    "    of indices of profiles that start within both the day and time bounds. This \n",
    "    works from the passed dataframe of profile times.\n",
    "    '''\n",
    "    nprofiles = len(pDf)\n",
    "    pIndices = []\n",
    "    for i in range(nprofiles):\n",
    "        a0 = pDf[\"ascent_start\"][i]\n",
    "        if a0 >= date0 and a0 <= date1 + td64(1, 'D'):\n",
    "            delta_t = a0 - dt64(a0.date())\n",
    "            if delta_t >= time0 and delta_t <= time1: pIndices.append(i)\n",
    "    return pIndices\n",
    "```\n",
    "\n",
    "This function has both a date range and a time-of-day range. The resulting row index list corresponds\n",
    "to profiles that satisfy both time window constraints: Date and time of day. \n",
    "\n",
    "\n",
    "The end-result is this: We can go from a conceptual { time window } to a list of { metadata rows }, i.e. a\n",
    "list of integer row numbers, using the above utility function. Within the metadata structure `p` we can \n",
    "use these rows to look up ascent / descent / rest times for profiles.\n",
    "At that point we have very specific { time window } boundaries for selecting data\n",
    "from individual profiles. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f2c44f-c7cc-46a9-9bcf-62f6dd7ffe20",
   "metadata": {},
   "source": [
    "### bash, text editor, git, GitHub\n",
    "### running a Jupyter notebook server (code and markdown)\n",
    "\n",
    "\n",
    "- I learn the basic commands of the `bash` shell; including how to use a text editor like `nano` or `vim`\n",
    "- I create an account at `github.com` and learn to use the basic `git` commands\n",
    "    - `git pull`, `git add`, `git commit`, `git push`, `git clone`, `git stash`\n",
    "    - I plan to spend a couple of hours learning `git`; I find good YouTube tutorials\n",
    "- I create my own GitHub repository with a `README.md` file describing my research goals\n",
    "- I set up a Jupyter notebook server on my local machine\n",
    "    - As I am using a PC I install WSL-2 (Windows Subsystem for Linux v2)...\n",
    "        - ...and install Miniconda plus some Python libraries\n",
    "- I clone my \"empty\" repository from GitHub to my local Linux environment\n",
    "- I start my Jupyter notebook server, navigate to my repo, and create a first notebook\n",
    "- I save my notebook and use `git add, commit, push` to save it safely on GitHub\n",
    "- On GitHub: Add and test a **`binder`** badge\n",
    "    - Once that works, be sure to `git pull` the modified GitHub repo back into the local copy\n",
    "\n",
    "\n",
    "\n",
    "### Ordering, retrieving and cleaning datasets from OOI\n",
    "\n",
    "\n",
    "At this point we do not have any data; so let's do that next. There are two important considerations. \n",
    "First: If the data volume will exceed 100MB: That is too much to keep in a GitHub repository. The\n",
    "data must be staged \"nearby\" in the local environment; outside the repository but accessible by\n",
    "the repository code, as in:\n",
    "\n",
    "\n",
    "```\n",
    "               ------------- /repo directory\n",
    "              /\n",
    "/home --------\n",
    "              \\\n",
    "               -------------- /data directory\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Second: Suppose the repo *does* contain (smaller) datasets, to be read by the code. \n",
    "If the intent is to use `binder` to make a sandbox version of the repo\n",
    "available, all significant changes to this code should be tested: First locally\n",
    "and then (after a `push` to GitHub) ***in `binder`***. This ensures that not too \n",
    "many changes pile up, breaking binder in mysterious and hard-to-debug ways.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now that we have a dataset let's open it up and examine it within a Notebook.\n",
    "The data are presumed to be in NetCDF format; so we follow common practice of\n",
    "reading the data into an `xarray Dataset` which is a composition of `xarray\n",
    "DataArrays`. There is a certain amount of learning here, particularly as this\n",
    "library shares some Python DNA with `pandas` and `numpy`. Deconstructing an\n",
    "`xarray Dataset` can be very challenging; so a certain amount of ink is devoted\n",
    "to that process in this repo.\n",
    "\n",
    "#### Open and subset a NetCDF data file via the `xarray Dataset`  \n",
    "\n",
    "\n",
    "Data provided by OOI tends to be \"not ready for use\". There are several steps needed; and\n",
    "these are not automated. They require some interactive thought and refinement. \n",
    "\n",
    "\n",
    "- Convert the principal dimension from `obs` or `row` to `time` \n",
    "    - `obs/row` are generic terms with values running 1, 2, 3... (hinders combining files into longer time series)\n",
    "- Re-name certain data variables for easier use; and delete anything that is not of interest\n",
    "- Identify the time range of interest\n",
    "- Write a specific subset file\n",
    "    - For example: Subset files that are small can live within the repo\n",
    "\n",
    "\n",
    "```\n",
    "# This code runs 'one line at a time' (not as a block) to iteratively streamline the data\n",
    "\n",
    "#   Suggestion: Pay particular attention to the construct ds = ds.some_operation(). This ensures \n",
    "#     that the results of some_operation() are retained in the new version of the Dataset. \n",
    "\n",
    "ds = xr.open_dataset(filename)\n",
    "ds                                         # notice the output will show dimension as \"row\" and \"time\" as a data variable\n",
    "\n",
    "\n",
    "ds = ds.swap_dims({'row': 'time'})         # moves 'time' into the dimension slot\n",
    "ds = ds.rename({'some_ridiculously_long_data_variable_name':'temperature'})\n",
    "ds = ds.drop('some_data_variable_that_has_no_interest_at_this_point')\n",
    "\n",
    "\n",
    "ds = ds.dropna('time')                     # if any data variable value == 'NaN' this entry is deleted: Includes all\n",
    "                                           #   corresponding data variable values, corresponding coordinates and \n",
    "                                           #   the corresponding dimension value. This enables plotting data such\n",
    "                                           #   as pH that happens to be rife with NaNs. \n",
    "\n",
    "ds.z.plot()                                # this produces a simple chart showing gaps in the data record\n",
    "ds.somedata.mean()                         # prints the mean of the given data variable\n",
    "\n",
    "ta0 = dt64_from_doy(2021, 60)              # these time boundaries are set iteratively...\n",
    "ta1 = dt64_from_doy(2021, 91)              #   ...to focus in on a particular time range with known data...\n",
    "ds.sel(time=slice(ta0,  ta1)).z.plot()     #   ...where this plot is the proof\n",
    "\n",
    "\n",
    "ds.sel(time=slice(ta0,  ta1)).to_netcdf(outputfile)           # writes a time-bounded data subset to a new NetCDF file\n",
    "```\n",
    "\n",
    "#### Depth and time\n",
    "\n",
    "\n",
    "Datasets have a depth attribute `z` and a time dimension `time`. These are derived by the data \n",
    "system and permit showing sensor values (like temperature) either in terms of depth below the \n",
    "surface; or in time relative to some benchmark. \n",
    "\n",
    "#### Some complicating data features\n",
    "\n",
    "\n",
    "- Some signals may have dropouts: Missing data is usually flagged as `NaN`\n",
    "    - See the section above on using the xarray `.dropna(dimension)` feature to clean this up\n",
    "- Nitrate data also features ***dark sample*** data\n",
    "- Spectrophotometer instruments measure both ***optical absorption*** and ***beam attenuation***\n",
    "    - For both of these about 82 individual channel values are recorded\n",
    "        - Each channel is centered at a unique wavelength in the visible spectrum\n",
    "        - The wavelength channels are separated by about 4 nm\n",
    "        - The data are noisy\n",
    "        - Some channels contain no data\n",
    "    - Sampling frequency needed\n",
    "- Spectral irradiance carries seven channels (wavelengths) of data\n",
    "- Current measurements give three axis results: north, east, up\n",
    "    - ADCP details needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### wget\n",
    "\n",
    "\n",
    "`wget` can be used recursively to copy files from the web to local copies.\n",
    "`wget` used in the **Global Ocean** notebook to get 500MB of data from the \n",
    "cloud that would otherwise make the repository too bulky for GitHub.  \n",
    "\n",
    "\n",
    "Example usage, typically run from the command line, run from a Jupyter notebook\n",
    "cell, or placed in a `bash` script:\n",
    "\n",
    "\n",
    "```\n",
    "wget -q https://kilroybackup.s3.us-west-2.amazonaws.com/glodap/GLODAPv2.2016b.NO3.nc -O glodap/NO3.nc\n",
    "```\n",
    "\n",
    "The `-q` flag suppresses output ('quiet') and `-O` specifies the local name of the data file.\n",
    "\n",
    "\n",
    "\n",
    "### Basic Python as the baseline layer of the code\n",
    "\n",
    "\n",
    "\n",
    "#### Start by putting all code in notebooks\n",
    "\n",
    "\n",
    "\n",
    "#### Notebooks can become over-long or very code-dense so...\n",
    "\n",
    "\n",
    "\n",
    "##### Be prepared to break notebooks apart into smaller notebooks\n",
    "\n",
    "\n",
    "\n",
    "##### Be prepared to migrate blocks of working code to module files\n",
    "\n",
    "\n",
    "\n",
    "### Install Python data science libraries for plotting, opening datasets, slicing\n",
    "\n",
    "\n",
    "\n",
    "#### starting with `matplotlib`, `numpy`, `pandas`, and `xarray`\n",
    "\n",
    "\n",
    "\n",
    "### As needed bring in Python extension libraries\n",
    "\n",
    "\n",
    "\n",
    "#### interactive widgets, maps, animation, color maps\n",
    "\n",
    "\n",
    "\n",
    "### Pulling other data (besides shallow profiler) from the OOI data system\n",
    "\n",
    "\n",
    "\n",
    "### Pulling datasets from other programs: ARGO, MODIS, GLODAP, ROMS, MSLA, etcetera\n",
    "\n",
    "\n",
    "\n",
    "### Using binder as an ephemeral executable sandbox\n",
    "\n",
    "\n",
    "\n",
    "### Working from larger extra-repo datasets\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac125fb-2631-4eb4-8ca3-9644de476675",
   "metadata": {},
   "source": [
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's make a time subset of the dataset and plot the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's focus on a profiler.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's animate a time series.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Data product levels\n",
    "\n",
    "\n",
    "The \n",
    "[OOI Data Catalog Documentation](https://dataexplorer.oceanobservatories.org/help/overview.html#data-products) \n",
    "describes three levels of data product, summarized: \n",
    "\n",
    "\n",
    "* Level 1 ***Instrument deployment***: Unprocessed, parsed data parameter that is in instrument/sensor \n",
    "units and resolution. See note below defining a *deployment*. This is not data we are interested in using, as a rule.\n",
    "\n",
    "\n",
    "* Level 1+ ***Full-instrument time series***: A join of recovered and telemetered \n",
    "streams for non-cabled instrument deployments. For high-resolution cabled and recovered data, this product is \n",
    "binned to 1-minute resolution to allow for efficient visualization and downloads for users that do not need \n",
    "the full-resolution, gold copy (Level 2) time series. We'd like to hold out for 'gold standard'.\n",
    "\n",
    "\n",
    "* Level 2 ***Full-resolution, gold standard time series***: The calibrated full-resolution dataset \n",
    "(scientific units). L2 data have been processed, pre-built, and served \n",
    "from the OOI system to the \n",
    "[OOI Data Explorer](https://dataexplorer.oceanobservatories.org/)\n",
    "and to Users. The mechanisms are THREDDS and ERDDAP; file format  \n",
    "NetCDF-CF. There is one file for every instrument, stream, and deployment.  For more refer to this\n",
    "[Data Download](https://dataexplorer.oceanobservatories.org/help/overview.html#download-data-map-overview) link.\n",
    "\n",
    "\n",
    "\n",
    "## OOI terminology\n",
    "\n",
    "\n",
    "\n",
    "- **instrument**: A physical device with one or more sensors.\n",
    "- **stream**: Sensor data.\n",
    "- **deployment**: The act of putting infrastructure in the water, or the length of \n",
    "time between a platform going in the water and being recovered and brought back to shore.There are \n",
    "multiple deployment files per instrument. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Retained from prior iterations\n",
    "\n",
    "\n",
    "\n",
    "The sequence of events so far:\n",
    "\n",
    "\n",
    "* order data\n",
    "* clean the data to regular 1Min samples\n",
    "* scan the data for profiles; write these to CSV files\n",
    "* load in a profile list for a particular site and year\n",
    "\n",
    "\n",
    "Now we start charting this data. We'll begin with six signals, three each from the CTD and the fluorometer. \n",
    "Always we have two possible axes: Depth and time. Most often we chart against depth using the y-axis and \n",
    "measuring from a depth of 200 meters at the bottom to the surface at the top of the chart. \n",
    "\n",
    "\n",
    "CTD signals\n",
    "\n",
    "\n",
    "* Temperature\n",
    "* Salinity\n",
    "* Dissolved oxygen\n",
    "\n",
    "\n",
    "Fluorometer signals\n",
    "\n",
    "\n",
    "* CDOM: Color Dissolved Organic Matter)\n",
    "* chlor-a: Chlorophyll pigment A\n",
    "* scatt: Backscatter\n",
    "\n",
    "\n",
    "The other sensor signals will be introduced subsequently. These include nitrate concentration,\n",
    "pH, pCO2, PAR, spectral irradiance, local current and water density. \n",
    "\n",
    "\n",
    "Some profile pandas dataframe code; and filtering: \n",
    "\n",
    "\n",
    "```\n",
    "# Create a pandas DataFrame: Six columns of datetimes for a particular year and site\n",
    "#   The six columns are start/end for, in order: ascent, descent, rest: See column labels below.\n",
    "def ReadProfiles(fnm):\n",
    "    \"\"\"\n",
    "    Profiles are saved by site and year as 12-tuples. Here we read only\n",
    "    the datetimes (not the indices) so there are only six values. These\n",
    "    are converted to Timestamps. They correspond to ascend start/end, \n",
    "    descend start/end and rest start/end.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(fnm, usecols=[\"1\", \"3\", \"5\", \"7\", \"9\", \"11\"])\n",
    "    df.columns=['ascent_start', 'ascent_end', 'descent_start', 'descent_end', 'rest_start', 'rest_end']\n",
    "    df['ascent_start'] = pd.to_datetime(df['ascent_start'])\n",
    "    df['ascent_end'] = pd.to_datetime(df['ascent_end'])\n",
    "    df['descent_start'] = pd.to_datetime(df['descent_start'])\n",
    "    df['descent_end'] = pd.to_datetime(df['descent_end'])\n",
    "    df['rest_start'] = pd.to_datetime(df['rest_start'])\n",
    "    df['rest_end'] = pd.to_datetime(df['rest_end'])\n",
    "    return df\n",
    "\n",
    "\n",
    "# FilterSignal() operates on a time series DataArray passed in as 'v'. It is set up to point to multiple possible\n",
    "#   smoothing kernels but has just one at the moment, called 'hat'.\n",
    "def FilterSignal(v, ftype='hat', control1=3):\n",
    "    \"\"\"Operate on an XArray data array (with some checks) to produce a filtered version\"\"\"\n",
    "    # pre-checks\n",
    "    if not v.dims[0] == 'time': return v\n",
    "\n",
    "    if ftype == 'hat': \n",
    "        n_passes = control1        # should be a kwarg\n",
    "        len_v = len(v)\n",
    "        for n in range(n_passes):\n",
    "            source_data = np.copy(v) if n == 0 else np.copy(smooth_data)\n",
    "            smooth_data = [source_data[i] if i == 0 or i == len_v - 1 else \\\n",
    "                0.5 * source_data[i] + 0.25 * (source_data[i-1] + source_data[i + 1]) \\\n",
    "                for i in range(len_v)]\n",
    "        return smooth_data\n",
    "    return v\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d94d45e",
   "metadata": {},
   "source": [
    "# Spectral Irradiance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> ***If we shadows have offended,***<BR>\n",
    "> ***Think but this, and all is mended,***<BR>\n",
    "> ***That you have but slumber'd here***<BR>\n",
    "> ***While these visions did appear.***<BR>\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "    \n",
    "This notebook should run in **`binder`**. It uses small datasets stored within this repo.\n",
    "\n",
    "The notebook charts\n",
    "CTD data, dissolved oxygen, nitrate, PAR, spectral irradiance, fluorescence and pH in relation\n",
    "to pressure/depth. The focus is\n",
    "shallow (photic zone) profilers from the Regional Cabled Array component of OOI.\n",
    "Specifically the Oregon Slope Base site in 2019. Oregon Slope Base is an instrumentation\n",
    "site off the continental shelf west of the state of Oregon.\n",
    "\n",
    "\n",
    "\n",
    "# Visions of the Photic Zone: CTD and other low data rate sensors\n",
    "\n",
    "\n",
    "The 'photic zone' is the upper layer of the ocean regularly illuminated by sunlight. This set of photic zone \n",
    "notebooks concerns sensor data from the surface to about 200 meters depth. Data are acquired from two to nine\n",
    "times per day by shallow profilers. This notebook covers CTD (salinity \n",
    "and temperature), dissolved oxygen, nitrate, pH, spectral irradiance, fluorometry and photosynthetically \n",
    "available radiation (PAR).  \n",
    "\n",
    "\n",
    "Data are first taken from the Regional Cabled Array shallow profilers and platforms. A word of explanation here: The\n",
    "profilers rise and then fall over the course of about 80 minutes, nine times per day, from a depth of 200 meters\n",
    "to within about 10 meters of the surface. As the ascend and descend they record data. The resting location in\n",
    "between these excursions is a platform 200 meters below the surface that is anchored to the see floor. The platform\n",
    "also carries sensors that measure basic ocean water properties.\n",
    "\n",
    "\n",
    "<BR>\n",
    "<img src=\"./images/vessels/revelle.jpg\" style=\"float: left;\" alt=\"ship and iceberg photo\" width=\"900\"/>\n",
    "<div style=\"clear: left\"><BR>\n",
    "\n",
    "\n",
    "Research ship Revelle in the southern ocean: 100 meters in length. \n",
    "Note: Ninety percent of this iceberg is beneath the surface. \n",
    "\n",
    "\n",
    "More on the Regional Cabled Array oceanography program [here](https://interactiveoceans.washington.edu).\n",
    "    \n",
    "    \n",
    "### Study site locations\n",
    "    \n",
    "\n",
    "We begin with three sites in the northeast Pacific: \n",
    "    \n",
    "\n",
    "```\n",
    "Site name               Lat               Lon\n",
    "------------------      ---               ---\n",
    "Oregon Offshore         44.37415          -124.95648\n",
    "Oregon Slope Base       44.52897          -125.38966 \n",
    "Axial Base              45.83049          -129.75326\n",
    "```   \n",
    "\n",
    "\n",
    "## Spectral Irradiance\n",
    "\n",
    "* The data variable is `spkir_downwelling_vector` x 7 wavelengths per below\n",
    "* 9 months continuous operation at about 4 samples per second gives 91 million samples\n",
    "* DataSet includes `int_ctd_pressure` and `time` Coordinates; Dimensions are `spectra` (0--6) and `time`\n",
    "* Oregon Slope Base `node : SF01A`, `id : RS01SBPS-SF01A-3D-SPKIRA101-streamed-spkir_data_record`\n",
    "* Correct would be to plot these as a sequence of rainbow plots with depth, etc\n",
    "\n",
    "See [Interactive Oceans](https://interactiveoceans.washington.edu/instruments/spectral-irradiance-sensor/): \n",
    "\n",
    "\n",
    "> The Spectral Irradiance sensor (Satlantic OCR-507 multispectral radiometer) measures the amount of \n",
    "> downwelling radiation (light energy) per unit area that reaches a surface. Radiation is measured \n",
    "> and reported separately for a series of seven wavelength bands (412, 443, 490, 510, 555, 620, \n",
    "> and 683 nm), each between 10-20 nm wide. These measurements depend on the natural illumination \n",
    "> conditions of sunlight and measure apparent optical properties. These measurements also are used \n",
    "> as proxy measurements of important biogeochemical variables in the ocean.\n",
    ">\n",
    "> Spectral Irradiance sensors are installed on the Science Pods on the Shallow Profiler Moorings \n",
    "> at Axial Base (SF01A), Slope Base (SF01A), and at the Endurance Array Offshore (SF01B) sites. \n",
    "> Instruments on the Cabled Array are provided by Satlantic â€“ OCR-507. \n",
    "\n",
    "\n",
    "```\n",
    "spectral_irradiance_source = './data/rca/irradiance/'\n",
    "ds_irradiance = [xr.open_dataset(spectral_irradiance_source + 'osb_sp_irr_spec' + str(i) + '.nc') for i in range(7)]\n",
    "\n",
    "# Early attempt at using log crashed the kernel\n",
    "\n",
    "day_of_month_start = '25'\n",
    "day_of_month_end = '27'\n",
    "time0 = dt64('2019-01-' + day_of_month_start)\n",
    "time1 = dt64('2019-01-' + day_of_month_end)\n",
    "\n",
    "spectral_irradiance_upper_bound = 10.\n",
    "spectral_irradiance_lower_bound = 0.\n",
    "ds_irr_time_slice = [ds_irradiance[i].sel(time = slice(time0, time1)) for i in range(7)]\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(12,8), tight_layout=True)\n",
    "colorwheel = ['k', 'r', 'y', 'g', 'c', 'b', 'm']\n",
    "for i in range(7):\n",
    "    axs.plot(ds_irr_time_slice[i].spkir_downwelling_vector, \\\n",
    "                ds_irr_time_slice[i].int_ctd_pressure, marker='.', markersize = 4., color=colorwheel[i])\n",
    "    \n",
    "axs.set(xlim = (spectral_irradiance_lower_bound, spectral_irradiance_upper_bound), \\\n",
    "        ylim = (60., 0.), title='multiple profiles: spectral irradiance')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f0e24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e93776-dd0b-45c2-b062-a1ae479eb6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760eecce-504c-402f-8a7c-899748d86d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7464b38c-4c4f-48b7-b86c-34f55e7a70f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988eccb1-08f9-4761-b1ae-1d09428bc14d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac6b69b-cdcc-4dee-9d13-3f5e3e7d7c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stand-alone code to plot a user-specified mooring extraction.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "moor_fn = Path('/Users/pm8/Documents/LO_output/extract/cas6_v3_lo8b/'\n",
    "    +'moor/ooi/CE02_2018.01.01_2018.12.31.nc')\n",
    "\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load everything using xarray\n",
    "ds = xr.load_dataset(moor_fn)\n",
    "ot = ds.ocean_time.values\n",
    "ot_dt = pd.to_datetime(ot)\n",
    "t = (ot_dt - ot_dt[0]).total_seconds().to_numpy()\n",
    "T = t/86400 # time in days from start\n",
    "print('time step of mooring'.center(60,'-'))\n",
    "print(t[1])\n",
    "print('time limits'.center(60,'-'))\n",
    "print('start ' + str(ot_dt[0]))\n",
    "print('end   ' + str(ot_dt[-1]))\n",
    "print('info'.center(60,'-'))\n",
    "VN_list = []\n",
    "for vn in ds.data_vars:\n",
    "    print('%s %s' % (vn, ds[vn].shape))\n",
    "    VN_list.append(vn)\n",
    "    \n",
    "# populate lists of variables to plot\n",
    "vn2_list = ['zeta']\n",
    "if 'shflux' in VN_list:\n",
    "    vn2_list += ['shflux', 'swrad']\n",
    "vn3_list = []\n",
    "if 'salt' in VN_list:\n",
    "    vn3_list += ['salt', 'temp']\n",
    "if 'oxygen' in VN_list:\n",
    "    vn3_list += ['oxygen']\n",
    "\n",
    "# plot time series using a pandas DataFrame\n",
    "df = pd.DataFrame(index=ot)\n",
    "for vn in vn2_list:\n",
    "    df[vn] = ds[vn].values\n",
    "for vn in vn3_list:\n",
    "    # the -1 means surface values\n",
    "    df[vn] = ds[vn][:, -1].values\n",
    "\n",
    "plt.close('all')\n",
    "df.plot(subplots=True, figsize=(16,10))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
